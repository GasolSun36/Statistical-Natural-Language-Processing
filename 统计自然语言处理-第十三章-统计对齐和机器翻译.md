### 统计自然语言处理-第十三章-统计对齐和机器翻译

​	第十三章主要内容是统计对齐和机器翻译。这一章讲的是用统计学的方法进行机器翻译，当然这个技术在$Deep Learning$大规模使用之前(即$word2vec，transformer$等)是主流的工作，但是效果可以说非常差，鲁棒性不算很高，不过思想很重要，对自己以后的科研会有帮助。

------

<!--more-->

​	本章的重点在于**文本对齐**、词对齐，然后是**统计翻译系统**的构建过程。

#### 1.  基本概念和介绍

​	机器翻译就是自动地将文本或者谈话内容从一种语言翻译为另外一种语言，属于$NLP$的最重要的应用领域之一。做机器翻译的方法很多，有**直接翻译法、句法转换法、语义转换法、中间语言法**等。直接翻译法是最简单的方法，即“词”对“词”的对齐翻译方式，从源语言的的句子出发，按词翻译成目标语言。这个方法有很明显的缺陷，包括不同语言之间可能不存在一一对应的映射关系、语序问题等。句法转换法在直接翻译法的基础上，可以解决其词序的问题，即用手工定义的规则去转换成目标语言的**句法树**，并在句法树的基础上生成目标语言的翻译。当然句法转换法也有缺陷，就是两个语言之间可能有不同的语法结构，导致翻译出来的句子晦涩难懂。语义转换法将原文转换成语义表示形式，可以解决句法转换法的问题，但是也是有缺陷，即使字面意思完全准确，但得到的句子用户可能还是不理解。**中间语言法**是独立于任何语言的知识表示形式，其优点在于进行多语种翻译时，只需对每种语言开发一个分析模块和转换模块即可，模块总数为$O(n)$而对于上述三个转换方法，对于每两两的语言都需要开发一个转换模块，模块总数为$O(n^2)$。

#### 2. 文本对齐

​	一些国家的官方语言可能会有很多种，我们可以用这些官方文件作为**平行语料库**，这是一个很好的数据集。在已知双语文本的情况下，首要任务是对齐文本，确定原文和译文句子或者段落之间的对应关系。确立了关系后，下一步是寻找词之间的对应。

​	首先明确下**句珠**的概念。句子对齐问题就是将源语言中的一组句子和目标语言中的一组句子进行对应的过程，每组句子可以为空，也可以额外加入对应源语言中不存在句子，或者删除原有的句子，我们称这两组**对应的句子**为一个句珠。最常见的是$1:1$句珠，当然也有$m:n$句珠，只不过不常见，有的时候不同规模的句珠可以相互对齐。

​	文本对齐的方式大体有三种，分别是基于长度的方法、基于词语对齐或字符串对齐的方法和基于偏移位置对齐的方法。在对齐方式中又可以分为位置之间的平均对齐和文本中某些特定位置的对齐。$S$代表源语言，$T$代表目标语言，它们都是由单个句子组成的集合，即$S=(s_1,s_2,...,s_I),T=(t_1,t_2,...,t_J)$。

##### 		2.1基于长度的对齐算法

​		基于长度的对齐方法基本思想是假设$S$和$T$存在比例关系，即短句对应于短句，长句对应于长句。句子长度我们可以定义为句子中的单词或字符的长度。课本上讲解了三个具体的方法，在这里我们只讲解第一个。

​		这个方法的目的是使下式的概率值取最大值，$A$代表了对齐方式，公式为：

​		$arg\max\limits_A P(A|S,T)=arg\max_A P(A,S,T)$

​		为了计算这个概率值，将对齐的文本分解为句珠序列$(B_1,B_2,...B_k)$，那么：

​		$P(A,S,T) \approx \prod^K\limits_{k=1}P(B_k)$

​		公式明确，接下里的任务就是在句珠内句子已知的情况下，估算某一类句珠的概率值(比如$1:1,2:2$等类型句珠)。当然，在这个方法中，他们假设句子只有五类句珠模式，即$\{1;1,1:0,0:1,2:1,1:2,2:2\}$。并假设$D(i,j)$表示两组句子之间对齐的最小耗费函数，并假设$D(0,0)=0$，这样我们就可以通过动态规划计算。计算公式为：

$$
D(i,j) =min
\begin{cases}
D(i,j-1)+cost(0:1\ align\ \emptyset,t_j)  \\
D(i-1,j)+cost(1:0\ align\ s_i,\emptyset)  \\
D(i-1,j-1)+cost(1:1\ align\ s_i,t_j)  \\
D(i-1,j-2)+cost(1:2\ align\ s_i,t_{t-1},t_j)  \\
D(i-2,j-1)+cost(2:1\ align\ s_{-1},s_i,t_j)  \\
D(i-2,j-2)+cost(2:2\ align\ s_{-1},s_i,t_{j-1},t_j)  \\
\end{cases}
$$
​		 这样，我们就可以用动态规划高效的计算出$D(i,j)$。当然我们还要计算每类对齐的耗费。耗费的计算基于的是句珠中每一种语言句子的长度$l_1,l_2$，即句子中的字符数。我们定义新变量：

​		$\delta=(l_2-l_1\mu)/\sqrt{l_1s^2}$

​		耗费计算函数为：

​		$cost(l_1,l_2)=-logP(\alpha\ align|\delta(l_1,l_2,\mu,s^2))$

​		其中$\alpha\ align$代表了某种可能的对齐形式(例如$1:1,2:2$)，见上面所述。这里值可以通过统计学的方式获取到，当然$1:1$模式肯定是较高的概率值。

##### 	2.2 基于信号处理技术的偏移位置对齐算法

​		这种方法没有试图对齐句子，而是在平行文本中利用位置偏移量的概念，即$S$和$T$中一定位置的文本是大致对齐的。其使用了**同源词**信息。同源词是指不同语言中的词由于借鉴或者来源于同一种祖先语言而具有的**相同特征**。

比如英语的$superior$和法语的$sup\acute{e}rieur$，我们可以通过字符序列的**相似程度**来寻找同源词。

​		这个方法首先将$S$和$T$连接起来，构建一个点阵图，如下图，其中左下方和右上方两条对角线代表了$S和T$的匹配关系，所以它们的对齐非常重要。但是这种方法又有缺陷，即它的假设前提是$S$和$T$存在很多同源词，而应用到英语-中文翻译中，基本上同源词出现会非常少甚至没有。

##### 	2.3 句子对齐的词汇方法

​		句子中的词能给句子对齐提供很多有价值的信息，所以这种方式非常重要。这种方法使用词语级别的部分对齐来推导出句子级别的对齐的**最大似然估计方法**，同时又反过来优化词语对齐。这个方法书上内容很多，这里不过多介绍，参考课本301页。

##### 	2.4 小结

​		如果我们使用的是比较干净的文本，那么句子对齐在很多算法中会取得很好的结果。但是如果翻译都使用了意译法，并且源语言和目标语言相差较大时，基于**词模型**的方法会有很好的效果，其他模型效果一般。

#### 3. 词对齐

​	词对齐的主要思想是利用词对出现的频率，频率越高，被收入双语词典的可能性越大。但是在词对齐中会使用到很多先验知识，比如双语语料库。

#### 4. 统计机器翻译

​	统计机器翻译大体分为三个部分，语言模型、翻译模型和解码器。语言模型是给出生成某个英文句子的概率$P(e)$，在前面我们已经介绍过了基于$n-gram$和$PCFG$的语言模型。翻译模型，假设一个简单的，基于词对齐的翻译模型：

​	$P(f|e)=\frac{1}{Z}\sum^l\limits_{a_1=0}...\sum^l\limits_{a_m=0}\prod^m\limits_{j=1}P(f_j|e_{a_j})$

​	其中，$e$代表英文句子，$l$是**按词计算**的句子长度，$f$是法文句子，$m$是其长度，$f_j是f的第j个词$，$a_j是e中与f_i对齐的词的位置$，$e_{a_j}是e中与f_j对齐的词$，$P(w_f|w_e)$是翻译概率。这个式子表达的基本思想很直观，就是累加法语单词对齐英语单词所有情况的概率。每个英文单词可以对应多个法语单词，但每个法语单词只能对应一个英文单词。当句子对齐之后，对所有$m$个对齐概率求乘积作为对齐模式的概率。例如：

​	$P(Jean\ aime\ Marie|John\ loves\ Mary)$

​	其中$(Jean,John),(aime,loves),(Marie,Mary)$代表对齐成分，只需要想成对齐概率即可求出：

​	$P(f|e)=(Jean,John)\times(aime,loves)\times(Marie,Mary)$

​	对于解码器来说，它可以通过贝叶斯公式算出，即:

​	$\hat{e}=arg\max\limits_{e}P(e|f)=arg\max\limits_{e}\frac{P(e)P(f|e)}{P(f)}=arg\max\limits_{e}P(e)P(f|e)$

​	并且可以通过**启发式搜索算法(例如栈搜索法)**进行搜索。

​	此外，我们还要引入**富余度**的概念，它表示翻译时和一个英文单词对应的法语单词的数目。例如$farmers$对应于法语的$les\ agriculteurs$，所以它的富余度为2，但是绝大部分英文单词的富余度为1。

------

​	翻译模型面临如下几个问题：

- 富余度的不对称性。例如单个法语单词可以同时对应数个英语单词，例如英语单词$to\ go$翻译为法语时对应$aller$。
- 独立性假设。我们在构建概率模型时做了很多独立性假设，但是实际中不一定严格满足。
- 对训练数据的敏感性。例如训练数据取自于取料的不同部分，都会导致最后参数估计值的较大变化。
- 算法效率。算法处理长句会花费过多时间，所以直接去除30个词以上的句子。

当然，统计翻译系统还会有一些因为缺少语言学知识而导致的翻译错误：

- 没有引入短语概念。模型本身只和单个词语有关，但是如果能处理短语之间的关系，那么就会改善模型质量。
- 非局部依存。$n-gram$等模型只能捕捉局部信息，如果词之间存在长距离依存，那么模型效果也会大打折扣。
- 词态变化。模型中具备词态变化的词往往被区分为不同的词，这是一个缺陷。
- 数据稀疏问题。模型参数都是通过训练数据训练得到的，数据量少的情况参数误差会很大。

------

​	本章主要讲了统计机器翻译的一些基本概念和过程。这些概念和过程以前面几章作为基础。对于思想了解即可，而对于目前所用的深度学习模型，比如$transformer$，才是需要大家真正理解并贯通的地方。
