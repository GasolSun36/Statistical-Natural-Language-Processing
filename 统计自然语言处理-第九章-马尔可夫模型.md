### 统计自然语言处理-第九章-马尔可夫模型

​	第九章主要内容是介绍马尔可夫模型原理以及隐马尔可夫模型原理和他们的一些拓展。属于比较核心的章节，需要重点掌握和理解。之前在[Natural Language Processing with Probabilistic Models](https://github.com/MrSunCodes/-Natural-Language-Processing/tree/main/Natural%20Language%20Processing%20with%20Probabilistic%20Models)中通过隐马尔可夫模型实现了[Parts-of-Speech Tagging](https://github.com/MrSunCodes/-Natural-Language-Processing/blob/main/Natural%20Language%20Processing%20with%20Probabilistic%20Models/02_part-of-speech-tagging-and-hidden-markov-models/assignment/C2_W2_Assignment.ipynb)这个词性标注的小项目。

------

<!--more-->

​	本章，我们主要会讨论更复杂的词汇的句法和语义特征的获取，通过考察大型文本语料库中词汇的出现模式，设计一种算法和统计技术来填补现有电子词典的不足。

#### 1.  马尔可夫模型

​	一般说马尔可夫模型是指显式的马尔可夫模型，马尔可夫模型相当于一个弧上标明概率的有限状态自动机。在马尔可夫模型中，机器的当前状态是已知的，因此状态序列或者它的一些确定函数被认为是输出。根据事先定义好的状态序列，我们可以很简单的计算出某一个状态转移序列的概率：

​	$P(X_1,...,X_T)=P(X_1)P(X_2|X_1)P(X_3|X_1,X_2)...P(X_T|X_1,...,X_{T-1})$

​	$=P(X_1)P(X_2|X_1)P(X_3|X_2)...P(X_T|X_{T-1})$

​	$=\pi_{x_1}\prod^{T-1} \limits_{t=1} a_{x_tx_{t+1}}$

​	在实际应用中，最重要的一点在于能否把一个过程转化成一个马尔可夫过程，因为如果不经过映射通常不能直接应用。

#### 2. 隐马尔可夫模型(HMM)

​	隐马尔可夫模型是我们讲解的重点。在NLP中，当系统中表层事件可能是由底层事件引发的时候，HMM能有效地解决此类问题，最常见的例子就是**词性标注**。并且很重要的一点是，手动标注数据是件耗费时间和精力很大的事情，而使用HMM，在模型合适的情况下，可以直接通过**EM算法**(无监督学习方法)进行有效的训练。

​	由于课本讲解的方法不太容易懂，我这里在学习HMM的时候主要参考的是李航《统计学习方法》以及b站一个up主的推导：[李航统计学习之隐马尔可夫模型](https://www.bilibili.com/video/BV1B54y197jo?p=1)。所以一些符号是跟统计学习方法这本书同步的。

​	首先定义一些常用符号和集合：

​	1. 设$Q$是所有可能的状态的集合，$V$是所有可能的观测的集合：

​		$Q={q_1,q_2,...,q_N},V={v_1,v_2,...,v_M}$

​		其中$N$是可能的状态数，$M$是可能的观测数。

 2. 设$I$是长度为$T$的状态序列，$O$是对应的观测序列：

    $I=(i_1,i_2,...,i_T),O=(o_1,o_2,...,o_T)$

    这里长度概念就是时间轴，时间轴上共有$t=1 \to t=T$个时间点。

	3. 设$A$是状态转移概率矩阵，

    $A=[a_{ij}]_{N \times N}$

    其中：

    $a_{ij}=P(i_{t+1}=q_j|i_t=q_i),i=1,2,...,N;j=1,2,...,N$

    是指在时刻$t$处于状态$q_i$的条件下在时刻$t+1$转移到状态$q_j$的概率。

	4.  设$B$是观测概率矩阵：

    $B=[b_j(k)]_{N \times M}$

    其中：

    $b_j(k)=P(o_t=v_k|i_t=q_j),k=1,2,...,M;j=1,2,...,N$

    是指在时刻$t$处于状态$q_j$的条件下生成观测$v_k$的概率。

	5. 设$\pi$是初始状态概率向量：

    $\pi=(\pi_i)$

    其中：

    $\pi=P(i_1=q_i),i=1,2,...,N$

    是指$t=1$时刻处于状态$q_i$​的概率。

    ------

    

定义好了以上所用的符号和集合，我们可以进一步阐述HMM的**三要素**：

​	隐马尔科夫模型是由初始状态概率向量$\pi$、状态转移概率矩阵$A$和观测概率矩阵$B$决定，其中$\pi$和$A$决定状态序列，$B$决定观测序列，因此隐马尔可夫模型可以如下表示:

​	$\lambda=(A,B,\pi)$，$A,B,\pi$被称为HMM的三要素。

除此之外，还有HMM的**两个基本假设**：

​	1.**齐次马尔可夫性假设**。即假设隐藏的马尔可夫链在任意时刻$t$的状态只依赖于其前一时刻的状态，与其他时刻的状态及观测无关，也与时刻$t$无关。公式为：

​	$P(i_t|i_{t-1},o_{t-1},...,i_1,o_1)=P(i_t|i_{t-1}),t=1,2,...,T$

​	2.观测独立性假设。即假设任意时刻的观测只依赖于该时刻的马尔科夫链的状态，与其他观测及状态无关：

​	$P(o_i|i_T,o_T,i_{T-1},o_{T-1},...,i_{t+1},o_{t+1},i_t,i_{t-1},o_{t-1},...,i_1,o_1)=P(o_t|i_t)$

​	隐马尔可夫模型有**三个基本问题**：

​	1.**概率计算问题**。即给定模型和观测序列，计算在模型下观测序列出现的概率。

​	2.**学习问题**。即已知观测序列，估计模型参数，使得在该模型下观测序列概率最大。

​	3.**预测问题(解码问题)**。已知模型和观测序列，求最有可能的对应的状态序列。

​	后面的所有内容都是围绕着三个基本问题来的，也是为了解决这三个基本问题。

​	课本195有个**盒子和球模型**的例子，本书后面讲解前向和后向等算法的时候也使用了该例子，我会以该例子作为讲解，但是不再这里将该例子再阐述一遍。

#### 3. 概率计算算法

​	这个主要对应了第一个基本问题，包括了直接计算法、**前向算法以及后向算法**。

##### 	3.1. 直接计算法

​		已知模型和观测序列，直接用概率公式进行计算，列举所有可能的长度为$T$的状态序列$I$，求各个状态序列$I$和$O$的联合概率$P(O,I|\lambda)$，然后对所有可能的状态序列求和，最后得到$P(O|\lambda)$。

​		首先，我们需要知道的是$P(O|\lambda)$，而$P(O|\lambda)=\sum\limits_{I}P(O,I|\lambda)=\sum\limits_{I}P(O|I,\lambda)P(I|\lambda)$，证明如下：

> ​		$P(O,I|\lambda)=\frac{P(O,I,\lambda)}{P(\lambda)}=\frac{P(O|I,\lambda) \times P(\lambda,I)}{P(\lambda)}$
>
> ​		又因为：
>
> ​		$P(\lambda,I)=P(I|\lambda) \times P(\lambda)$
>
> ​		带回原式，可得：
>
> ​		$P(O,I|\lambda)=P(O|I,\lambda) \times P(I|\lambda)$

​		

​		因为$P(I|\lambda)=\pi_{i_1}a_{i_1i_2}a_{i_2i_3}...a_{i_{T-1}i_T},P(O,I|\lambda)=b_{i_1}(o_1)b_{i_2}(o_2)...b_{i_T}(o_T)$

​		所以：$P(O,I|\lambda)=P(O|I,\lambda) \times P(I|\lambda)=\pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)...a_{i_{T-1}i_T}b_{i_T}(o_T)$

​		又因为:$P(O|\lambda)=\sum\limits_{I}P(O,I|\lambda)$,实际上相当于把**边缘分布**转换成**联合分布**，方便后续处理。

​		所以：$P(O|\lambda)=\sum\limits_{I}P(O,I|\lambda)=\sum\limits_{i_1,i_2,...i_T}\pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)...a_{i_{T-1}i_T}b_{i_T}(o_T)$

​		我们可以看出，最终复杂度是$O(TN^T)$,对于计算不太友好，我们可以使用基于动态规划思想的前向传播算法来更好的计算$P(O|\lambda)$。

##### 	3.2.前向算法

​		这个算法其实非常符合直觉。我们通过计算初值、创建递推以及终止条件三步走就可以很简单很快速地计算出我们所要的观测序列概率$P(O|\lambda)$。

​		首先我们获得初值，公式如下：

​		$\alpha_t(i)=P(o_1,o_2,...,o_t,i_t=q_i|\lambda),i=1,2,...,N$

​		对于接下来的所有$\alpha$，创建递推式，即后续的$\alpha$要用到前面的$\alpha$：

​		$\alpha_{t+1}(i)=\lgroup \sum^N \limits_{j=1} \alpha_t(j)\alpha_{ji} \rgroup b_i(o_{t+1}),i=1,2,...,N$

​		所以每一步的$\alpha$都使用前一个$\alpha$，这样的思想就是动态规划的思想，将之前所有计算过的式子保留，并且等计算下一个式子时可以用到，以空间换时间，这样算法的效率得到提高。

​		最后，算法的目标可以通过所有$\alpha$求和得到，即：

​		$P(O|\lambda)=\sum^N \limits_{i=1} \alpha_T(i)$

​		其实这个算法才是比较常规的计算**HMM**观测序列概率的方法，之间做的Parts-of-Speech Tagging里就用了这种前向算法的方法进行了计算。前向算法的时间复杂度是$O(N^2T)$,比指数形式的直接计算快了很多。

​		我以例10.2中的例题为例，计算其中一些点的概率。

> ​		递推第一步，$\alpha_2(1)=(0.1*0.5+0.16*0.3+0.28*0.2)*0.5=0.077$，剩下的依次进行计算，需要注意的是，需要看准序列，比如$O=(红，白，红)$，那么计算$\alpha_2(i)$时我们需要乘的$b_i(o_2)$就需要是矩阵$B$的第二列，因为矩阵$B$是红、白的顺序排列的。

​		

##### 	3.3.后向算法

​		后向算法很想前向算法，只不过是从后往前计算，也是动态规划的思想。通过前向算法和后向算法二者结合，可以更有效地计算出观测序列的概率。

​		首先，初值我们将它设置为$\beta_T(i)=1$,因为最后他肯定会得到某个序列，是必然事件。随后是递推式，前向算法是很多个结点去更新一个结点，而后向算法是根据一个结点去更新很多个结点。

#### 4. 学习算法

​	学习算法主要用来解决第二个问题学习问题，即已知观测序列，估计模型参数，使得在该模型下观测序列概率最大。即目标是为了获得矩阵$A和B$这里主要有两种方法--**监督学习方法**和**无监督学习方法**。

##### 	4.1 监督学习方法

​		监督学习方法就是用统计学的角度，利用极大似然估计法来估计HMM的参数。对于状态转移概率$a_{ij}$来说，是

​		$a_{ij}=\frac{A_{ij}}{\sum^N \limits_{j=1}A_{ij}},i=1,2,...,N;j=1,2,...,N$

​		对于观测概率来说，是：

​		$b_{ij}=\frac{B_{jk}}{\sum^M \limits_{k=1}B_{jk}},j=1,2,...,N;k=1,2,...,M$

​		对于初始状态概率来说，是：

​		$\pi_i=S个样本中初始状态为q_i的频率$

​		综上，监督学习方法通过运用数据集中先前出现过的模式，去获得参数，这个方法是可以的，但是有时候参数可能会比统计来得更复杂一些。并且监督学习需要使用标注的训练数据，人工标注训练数据往往代价很高。

##### 	4.2 无监督学习方法

​		这里讲的无监督学习方法主要是**Baum-Welch算法**，它是由EM算法组成的，使用EM算法进行参数估计。EM算法我们还没学到，所以暂时空下。

#### 5. 预测算法

​	预测算法主要解决第三个问题，预测问题。即已知模型和观测序列，求最有可能的对应的状态序列$I$。书中主要介绍了两种方法，近似算法和维特比算法(Viterbi algorithm)。

##### 	5.1 近似算法

​		近似算法的思想是，在每个时刻$t$，选择在该时刻最有可能出现的状态$i_t^*$，从而的到一个状态序列$I^*=(i_1^*,i_2^*,...,i_T^*)$，并将它作为预测的结果。这个方法的好处在于非常简单，但由于缺点是其很像贪心算法的思想，导致每步的最优解不一定是全局最优解。其次，有可能存在转移概率为0的状态。但是，近似算法仍然很有用。

##### 	5.2 维特比算法

​		这个算法也是非常简单且直觉。首先初始求出得到每一个状态的概率$I_1$，然后建立递推式，每次从上次的状态中获得使其最大的状态，然后去乘发射概率并连线。循环下去直到最后一个状态截止。维特比算法还是运用了动态规划的思想从而降低运算的时间复杂度，我们还是以210页的球盒模型作为例子进行讲解。

​		$\delta_1(i)=\pi b_i(o_1)=\pi b_i(红),i=1,2,3$

​		带入实际数据后可得:

​		$\delta_1(1)=0.10,\delta_1(2)=0.16,\delta_1(3)=0.28$

​		$t=2$时，需要这么计算概率：

​		$\delta_2(i)=\max \limits_{1 \le j \le3}[\delta_1(j)a_{ji}]b_i(o_2)$

​		带入实际数据后可得：

​		$\delta_2(1)=\max \limits_{1 \le j \le3}[\delta_1(j)a_{ji}]$，$\delta_2(i)是依赖于\delta_1(j)的，正好就是动态规划的思想$

​		$=\max \limits_{j} \{0.10 \times 0.5,0.16 \times 0.3,0.28 \times 0.2\} \times0.5=0.028$

​		同理，$\delta_2(2)=0.0504,\delta_2(3)=0.042$

​		当算出最后的状态序列$\delta_T(i)$后，以$P^*$来表示最优路径的概率，则：

​		$P^*=\max \limits_{1 \le j \le3} \delta_3(i)=0.0147$，通过回溯法或者画图法，我们可以获得最终的状态序列。

------

#### 6. 总结

​	这一章比较难，重点在于三个基本问题，两个基本假设和一个模型的三要素。对三个基本问题的方法也分别是一个最基础的方法和一个提高方法。建议多读几遍书以更好的去理解隐马尔可夫模型。
