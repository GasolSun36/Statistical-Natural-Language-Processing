### 统计自然语言处理-第八章-词汇获取

​	第八章主要内容是考虑更复杂的词汇的句法和语义特征的获取。主要包括了评价方法、动词子范畴、附着歧义、选择倾向以及词汇之间的语义相似性。

------

<!--more-->

​	本章，我们主要会讨论更复杂的词汇的句法和语义特征的获取，通过考察大型文本语料库中词汇的出现模式，设计一种算法和统计技术来填补现有电子词典的不足。

#### 1.  评价方法

​	信息检索中的评价方法经常使用精确率($precision$)和召回率($recall$)。我们可以通过选择和目标集合构成一个$2\times2$的联立矩阵($contingency\ matrix$)

|                 | 目标     |               |
| --------------- | -------- | ------------- |
| **选择**        | $target$ | $\neg target$ |
| $selected$      | $tp$     | $fp$          |
| $\neg selected$ | $fn$     | $tn$          |

图片8.1

通过联立矩阵，求出$tp$($true\ positive$)、$fp$($false\ positive$)、$fn$($false\ negative$)、$tn$($true\ negative$)。

公式如下面所展示：

$precision=\frac{tp}{tp+fp}$，$recall=\frac{tp}{tp+fn}$。我们其实可以从联立矩阵中看出，精确率就是$tp$除以$selected$,而召回率就是$tp$除以$target$。

定义好了精确率和召回率，我们可以将其统一到一个度量尺度中，就是$F$测量,公式为：

$F=\frac{1}{\alpha^{\frac{1}{P}}+(1-\alpha)^{\frac{1}{R}}}$,其中$\alpha$是确定精确率和召回率权重的因子。$\alpha=0.5$对应着选择相同的权重给$P$和$R$,此时：

$F=\frac{2PR}{P+R}$

后续，作者还讨论了一些关于其他测量，比如准确率($accuracy$)以及错误率($error$)，以及漏识率($fallout$)，这里我们不再细究，因为主要运用到的还是$F$测量以及准确率和召回率。

#### 2. 动词子范畴

​	首先明确动词子范畴的定义：动词可以被划分为不同的句法范畴，也就是说，动词可以用不同的句法形式来表示自己的语义对象，这种句法范畴集合就是动词子范畴框架。

​	了解动词的子范畴框架对句法分析非常有用，比如看下面两句话：

​	$a.\ She\ told\ the\ man\ where\ Peter\ grew\ up.$

​	$b. She\ found\ the\ place\ where\ Peter\ grew\ up.$

​	如果知道了tell有一个子范畴框架：$NP\ NP\ S$，即主语、宾语和从句，并且知道find没有这么一个子范畴框架，但是find有另外一个子范畴框架:$NP\ NP$，即主语、宾语，我们就能对$where$从句的所属问题得出正确的结论。在a句子中，$where$从句属于$told$，第二个句子中，$where$从句属于$place$。解释如下：

​	$a.\ She\ told\ [the\ man]\ [where\ Peter\ grew\ up].$

​	$b. She\ found\ [the\ place\ [where\ Peter\ grew\ up]].$

​	这样来看，我们可以很好的运用子范畴框架去解决宾语消岐的问题。但是，大部分的词典中都没有子范畴框架的信息。接下来我们要介绍一个算法，叫$Lerner$，它是用于基于一个语料库来判断动词$v$是否有框架$f$。它的算法有两步决策：

 	1. **暗示(cue)**。暗示是包含词语和句法范畴的正则模式，通过这些正则模式来发现子范畴框架。对于某一个特定的暗示$c^j$，我们定义了一个错误概率$\epsilon_j$，表示如果在暗示$c^j$的基础上为动词$v$指定框架$f$,那么错误的可能性有多大。
 	2. **假设检验**。基本思想是：我们最初假设框架不适合这个动词，并把它作为零假设$H_0$，如果暗示$c^j$指示$H_0$错误的概率很高，那么我们可以拒绝这个假设。

​    以这句话为例：$I\ came\ Thursday,\ before\ the\ storm\ started.$

​	这个对应的模式为:$came-V,Thursday-CAP, \ ,PUNC$。这种情况是非常少见的，因为当一个动词接一个大写字母开头的词，并且在接一个标点符号时，它几乎总是只有一个宾语而且不再需要其他的句法对象。所以当暗示出现时，动词具有框架$NP,NP$的错误概率很低。

​	当框架的暗示定义好后，我们就可以对语料库进行分析，并且对于任意动词和框架的组合，可以统计出这个框架的暗示和这个动词同现的次数。假设动词$v^i$在语料库中总共出现了$n$次，并且有$m$次($m \leq n$)是和框架$f^j$的暗示同现的。那么我们就可以给出下面的错误概率拒绝零假设$H_0$,这个零假设就是$v^i$不具有框架$f^j$。

​	$p_E=\sum\limits_{r=m}^n C_n^r \epsilon_j^r(1-\epsilon_j)^{n-r}$

​	其中，$\epsilon_j$是$c_j$的错误概率。这个概率是指和$c^j$同现但不具有框架$f^j$的动词出现的概率。

​	高精准率和低召回率是我们使用假设检验的必然结果。我们仅仅寻找已经被很好地证明了的子范畴框架。相反，这也意味着我们不会理会那些出现极少的子范畴框架。

#### 3. 附着歧义

​	附着歧义问题是NLP的一个非常主要的问题，在分析句子时，有些短语可以附着在语法树的两个和两个以上节点，因此我们不得不确定这些短语的附着关系，比如下面一句话：

​	$The\ children\ ate\ the\ cake\ with\ a\ spoon.$

​	介词短语$with\ a\ spoon$在句子中的附着的位置不同，其含义也不同：如果其附着在$children$，那么可以理解为孩子们用勺子吃蛋糕，或者是孩子吃的是一个插着$spoon$的蛋糕即介词短语附属在$cake$上。后一种理解非常奇怪，但是是有可能的，我们人类凭借学习经验可以很快速的判断出，后者几乎不可能，但是计算机处理这个问题却不是很简单。

​	下面详细描述附着歧义问题：如果句子中介词短语出现在**作为宾语的名词短语**后面，一般都会产生这种类型的句法结构歧义。正是这种歧义结构导致例句存在不止一棵句法分析树，也就导致了现在的歧义问题。这种问题的解决思路是，在大部分情况下，利用简单的词汇统计，包括基于动词和前置词的同现计数和基于名词和前置词的同现计数。举个例子，比如下面这段话：

​	$Moscow\ sent\ more\ than\ 100000\ soldiers\ into\ Afghanistan...$

在语料库中，我们可以找到很多into和send同现的例子，但是into和soldier同现的次数相对较少。那么我们就会有充分的理由相信以into开头的介词短语附着于send而不是soldier。

​	以上思想，即基于这些统计信息的模型使用了似然比$\lambda$：

​	$\lambda(v,n,p)= \frac {P(p|v)}{P|n}$

​	其中，$P(p|v)$表示带有p的介词短语出现在动词$v$后面的概率。$P(p|n)$表示带有$p$的介词短语出现在名词$n$后面的概率。$\lambda(v,n,p)>0$说明介词短语附着于动词；否则附着于名词。

> $\lambda(v,n,p)>0$即$\frac {P(p|v)}{P|n}$>1，即$P(p|v)>P(p|n)$。

​	这个简单的方法效果很好，但有时会因为忽略了**低附着偏向性**的情况而导致效果不好。低附着偏向性的意思是当介词短语既可以附着在动词也可以附着在名词时，它偏向于句法树中位置较低的节点。

##### 	    3.1 Hindle and Rooth 1993的概率模型

​		首先定义**事件空间**的概念。事件空间是满足句子中包含一个及物动词，有一个名词短语和在名词短语之后的介词短语的所有子句。为了减少模型的混乱度，每次只考虑**一个介词**，如果在句子中一次出现两个含有相同介词的介词短语，那么我们也只针对第一个介词构建模型。比如下面两句话：

​		$a.\ The\ road\ to\ London\ is\ long\ and\ winding.$

​		$b.\ She\ sent\ him\ into\ the\ nursery\ to\ gather\ up\ his\ toys.$

​		根据这两个句子，我们可以将$(road,to)$和$(send,into)$的计数值分别加1。

​		整个算法的过程是：

  1. 通过计算所有的非歧义情况，构建初始模型。

  2. 把这个初始模型应用到所有歧义情况中，通过比较$\lambda$和预先定义的阈值大小，做出相应的附着关系的判断。例如，$\lambda>2.0$表示动词附着关系，小于2.0表示名词附着关系。

  3. 在两种附着关系计数中均匀地分配其余的歧义情况。也就是说，对于每一个歧义情况，$C(v,p)$和$C(n,p)$各增加0.5。

     还是以下面例句为例，我们需要看一下$into$后面从句是跟在$sent$还是跟在$soldiers$：

     $Moscow\ sent\ more\ than\ 100000\ soldiers\ into\ Afghanistan...$

     首先估计似然率需要的两个概率值。

     $P(VA_{into}=1|send)=\frac{C(send,into)}{C(send)}=\frac{86}{1742.5} \approx 0.049$

     $P(NA_{into}=1|soldiers)=\frac{C(soldiers,into)}{C(soldiers)}=\frac{1}{1478} \approx 0.0007$

     所以，$P(NA_{into}=0|soldiers)=1-P(NA_{into}=1|soldiers) \approx 0.9993$

     似然比为:$\lambda(send,soldiers,into) \approx log_2 \frac{0.049 \times 0.9993}{0.0007} \approx 6.13$

     那么结果说明，动词附着关系是名词附着关系的$2^{6.13} \approx 70倍$,所以我们以统计学的角度来肯定，这个是into是跟随在动词send后面的宾语。

#####         3.2 介词短语附着的一般评论和其他问题

​		如果使用模型来解决附着歧义问题，可能会有如下限制：一是我们仅仅考虑了前置词和它要附着的名词和动词，又是其他信息也是很重要的，在获得比$v,n,p$更多的信息时，人类判断附着歧义的准确率可以提高大约5%。

​		另外一点是，一个包含3个及以上名词的名词短语或者是左分支结构$[[NN]N]$或者是右分支结构$[N[NN]]$。左分支结构情况粗略对应于介词短语附着动词的情况$[V\ N\ P]$，右分支的情况对应于介词短语附着名词的情况$[V[N\ P]]$。比如$door\ bell\ manufacturer$对应左分支，$woman\ aid\ worker$对应右分支。

#### 4. 选择倾向

​	我们在上面讨论的情况是属于一般情况，其实大部分动词更倾向于有特定的论元类型，比如动词$eat$的宾语倾向于食物类，$think$的主语倾向于人，$bark$的主语倾向于动物狗。这些论元约束和我们先前考虑的句法限制是类似的。

​	在NLP中，选择倾向的获取是相当重要的。如果我们的电子词典中缺少某个词，比如$durian$，那么我们可以从选择约束中得到它的部分疑似。比如在下面的句子中，我们可以判断出$durian$是一种食物。

​	$Susan\ had\ never\ eaten\ a\ fresh\ durian\ before.$

​	下面我们介绍一个选择倾向模型，这个模型用两个概念形式化了选择倾向性：**选择倾向强度**以及**选择关联**。选择倾向强度度量了动词约束它的直接宾语的强度。它被定义为**直接宾语的先验分布**和我们所求的**动词直接宾语的分布**之间的**相对熵**($KL$)。

​	除了定义两个概念外，我们还使用两个假设来简化模型，一是只考虑直接宾语的**中心词**，因为中心词是确定名词短语是否和动词匹配的关键部分。第二，不处理单独的名词，而是考虑名词类，比如食物类等。这样通过基于类的模型促进了模型的一般化，并且简化了参数估计。下面，我们定义选择倾向参数如下：

​	$S(v)=\sum_c P(c|v)log\frac{P(c|v)}{P(c)}$

​	基于选择倾向强度，我们可以定义动词v和词类c之间的选择关联如下:

​	$A(v,c)=\frac{P(c|v)log\frac{P(c|v)}{P(c)}}{S(v)}$

​	最后我们需要一个规则来指定关联强度和名词之间的关系。如果名词n只属于某一类别c，那么我们只需简单地定义$A(v,n) \overset{\text{def}}{=}A(v,c) $；如果名词属于很多类，那么它的关联强度就定义为它的所有类别中**关联强度最大**的值。

​	$A(v,n)=\max\limits_{c \in classes(n)}A(v,c)$

​	例如，下面的$chair$就有很多类别，比如$furniture$和$people$：

​	$Susan\ interrupted\ the\ chair.$

​	因为$A(interrupt,people) \gg A(interrupted,furniture)$，所以我们把关联强度$A(interrupt,chair)$建立在people类的基础上，因为打断一个人说话比打断一个家具更常用。通过这个方法，我们也对$chair$进行了消岐。这也引出了一个方法，即通过名词可能的类之间选择最高的关联强度进行**语义消岐**。

​	既然$A(v,c)$非常重要，那么我们已经知道了$S(v)$怎么算了，我们还需要了解$P(c|v)$怎么计算，公式如下：

​	$P(c|v)=\frac{P(v,c)}{P(v)},P(v)=\frac{c(v)}{\sum_{v^{'}}C(v^{'})}$

​	$P(v,c)=\frac{1}{N} \sum \limits_{n \in words(c) } \frac{1}{\abs{classes(n)}} C(v,n)$

​	上面的公式中，$N$是语料库中**动词-宾语对**的总数量，$words(c)$表示名词类$c$中所有名词的集合，$classes(n)$表示包含名词$n$的名词类的数目，$C(v,n)$表示动词$v$和以$n$为中心词的宾语名词短语对的数目。

#### 5. 语义相似性

​	语义相似性的意思是通过一些行为特点或特征将某个未知单词的语义跟其他已知单词联系起来，并获得它们的相似性或相似度。

​	语义相似性分为**相似性一般化**和**类一般化**。相似性一般化中，我们需要考虑的只是一个词的最相似的邻近词，而在类一般化中，我们要考虑和一个词有关联的一类词的特性。

​	在度量方法中，有**向量空间的度量方法**以及**概率度量方法**，我们最常用的是向量空间的度量方法。

​	对于向量空间的度量方法，可以看如下表格：

|  相似性测量   |                          定义                           |
| :-----------: | :-----------------------------------------------------: |
|   匹配系数    |                   $\abs{X \bigcap Y}$                   |
|  $Dice$系数   |      $\frac{2\abs{X \bigcap Y}}{\abs{X}+\abs{Y}}$       |
| $Jaccard$系数 |      $\frac{\abs{X \bigcap Y}}{\abs{X \bigcup Y}}$      |
|   交迭系数    |    $\frac{\abs{X \bigcap Y}}{\min(\abs{X},\abs{Y})}$    |
|   $cosine$    | $\frac{\abs{X \bigcap Y}}{\sqrt{\abs{X}\times\abs{Y}}}$ |

​	我们最常用的是$cosine$，因为余弦的特性，使得我们可以忽视数据量的差异而判断它们的相似度。

​	概率度量方法是通过条件概率的方式来统计词汇，这里不再赘述。
