### 统计自然语言处理-第十章-词性标注

​	第十章主要内容是使用统计学方法进行句子中每个词的词性标注，主要有马尔可夫模型、隐马尔科夫模型以及基于转换的思想。在这章最后还探讨了词性标注的作用等等。之前在[Natural Language Processing with Probabilistic Models](https://github.com/MrSunCodes/-Natural-Language-Processing/tree/main/Natural%20Language%20Processing%20with%20Probabilistic%20Models)中通过隐马尔可夫模型实现了[Parts-of-Speech Tagging](https://github.com/MrSunCodes/-Natural-Language-Processing/blob/main/Natural%20Language%20Processing%20with%20Probabilistic%20Models/02_part-of-speech-tagging-and-hidden-markov-models/assignment/C2_W2_Assignment.ipynb)这个词性标注的小项目。

------

<!--more-->

​	词性标注是一种有限语法消岐问题。许多次都不只有一种语法类别，即存在兼类的情况。在进行标注时，要努力确定对于句子中的一个词的特定用法，最可能是哪种语法类别。词性标注是一个比完整句法分析更有用、更容易控制的中间层表达。

#### 1.  标注中的信息源

​	关于如何查看文本中的一个词的词性是否正确，有两个基本信息源。

​	一是**观察我们感兴趣的词邻近上下文中的其他词的标注**。这些词的词性也可能是有歧义的，但是这里有一些基本的观测准则，**很多词性序列是很常见的**。例如$AT\ JJ\ NN$,然而有些词性序列因为不符合语法则基本上不可能，比如$AT\ JJ\ VBP$。

​	二是**利用词本身提供的信息**。例如$flour$可以被用做动词，但是它更多是作为名词出现的。这里有些学者们设计了一个$dumb$标注器，只是简单地把最常用的标注分配给了每个词，结果效果非常好：大概$90\%$。这个$dumb$标注器的性能也被用作后续研究的基准性能$baseline$。

​	在本章后面的方法中，我们主要考虑当前上下文中的词性，与此同时词性间词语的分布也提供了大量的附加信息。在大部分词语中，因为词语的不同特性其使用分布极不均匀。不均匀的分布也是导致我们预期用统计方法进行标注比用确定方法好的一个原因。

#### 2. 马尔可夫模型标注器

​	在使用马尔可夫模型标注器时，我们要用到两个马尔可夫模型的特质：即**有限视野**和**时间不变性**。我们假设一个词语的标记只依赖于前面的标记(有限视野),而且这种依赖并不随时间而改变(时间不变性)。

​	接下来我们定义一些需要用到的符号，这些符号在后续公式的推导中都会用到。

| $w_i$        | $语料库位置i处的词语$                                        |
| ------------ | ------------------------------------------------------------ |
| $t_i$        | $w_i的标记$                                                  |
| $w_{i,i+m}$  | $位置i与i+m之间出现的词语(或符号:w_i...w_{i+m},w_i,...,w_{i+m},w_{i(i+m)})$ |
| $t_{i,i+m}$  | $w_i...w_{i+m}的标记t_i...t_{i+m}$                           |
| $w^l$        | $词典中的第l个词$                                            |
| $t^j$        | $标注集中的第j个标记$                                        |
| $C(w^l)$     | $w^l在训练集中出现的次数$                                    |
| $C(t^j)$     | $t^j在训练集中出现的次数$                                    |
| $C(t^j,t^k)$ | $t^k跟随t^j之后在训练集中出现的次数$                         |
| $C(w^l:t^j)$ | $w^l被标注成t^j在训练集中出现的次数$                         |
| $T$          | $标注集中标记的个数$                                         |
| $W$          | 词典中词的个数                                               |
| $n$          | 句子长度                                                     |

------

​	我们通过**最大似然估计**来直接估计一个词被一个特定的状态(标记)发射出来的概率：

​	$P(w^l|t^j)=\frac{C(w^l,t^j)}{C(t^j)}$

​	应用贝叶斯法则，我们可以写成：

​	$arg\max\limits_{t_{1,n}}P(t_{1,n}|w_{1,n})=arg\max\limits_{t_{1,n}}\frac{P(w_{1,n}|t_{1,n})P(t_{1,n})}{P(w_{1,n})}=arg\max\limits_{t_{1,n}}P(w_{1,n}|t_{1,n})P(t_{1,n})$

​	通过设定两个假设，公式可以继续化简得：

​	$P(w_{1,n}|t_{1,n})P(t_{1,n})=\prod^n \limits_{i=1} P(w_i|t_{1,n}) \times P(t_n|t_{1,n-1}) \times P(t_{1,n-1}|t_{1,n-2}) \times ... \times P(t_2|t_1)$

​	$=\prod^n \limits_{i=1} P(w_i|t_i) \times P(t_n|t_{n-1}) \times P(t_{n-1}|t_{n-2}) \times ... \times P(t_2|t_1)$

​	$=\prod^n \limits_{i=1} [P(w_i|t_i) \times P(t_i|t_{i-1})]$

​	同时，我们定义$P(t_1|t_0)=1.0$来简化我们的表达式。

​	由此，我们在确定一个句子的最优标注的等式是：

​	$\hat{t}_{1,n}=arg\max\limits_{t_{1,n}}P(t_{1,n}|w_{1,n})=arg\max\limits_{t_{1,n}} \prod^n \limits_{i=1}P(w_i|t_i)P(t_i|t_{i-1})$

​	所以训练马尔可夫模型的算法就很简单了，总共有**两步**，如下所示：

​	1.计算两两tag之间出现的概率，即$P(t^k|t^j)=\frac{C(t^j,t^k)}{C(t^j)}$

​	2.计算tag和word之间出现的概率，即$P(w^l|t^j)=\frac{C(w^l,t^j)}{C(t^j)}$

​	这样我们就可以对马尔可夫模型进行训练，需要注意的是，在训练时我们最好使用一些平滑策略进行修订。

​	在课本220页，表10.3对应算法第一步进行后生成的结果，表10.4对应算法第二步进行后生成的结果。可以很清楚地看到，算法第一步是tag和tag之间，而第二步是tag和word之间。

------

​	模型训练完毕，那么怎么用模型来生成词性标注的序列呢？直接用确定句子最优标注的等式会使得复杂度是句子长度的指数倍，在这里我们使用前一章讲过的$Viterbi$算法进行生成。这里不再详细赘述。

​	对于这个模型来说，有很多变形，比如之前的马尔可夫模型只是**二元语法标注器**，在这里可以使用三元语法标注器，因为其记忆了两个标记，因此能够让我们区分更多情况。或者我们可以使用插值方法，将一元、二元和三元模型全部联合起来，比如:

​	$P(t_i|t_{1,i-1})=\lambda_1 P_1(t_i)+\lambda_2 P_2(t_i|t_{i-1})+\lambda_3 P_3(t_i|t_{i-1,i-2})$

​	这种方法实际上就等同于手工地扩充模型，使其在低阶模型记忆不足的情况下选择高阶状态。

#### 3. 隐马尔可夫模型

​	在这里需要明确一下**未登录词**的概念。有时候需要标注的句子中的一些词并没有出现在训练语料库中，有些词甚至不在字典里，这些词就是未登录词，即已有的语料库和标记库无法对其进行标注或预测。

​	未登录词最简单的处理模型是假设它们可以有任意词性，但是由于丢失了这些词语的词汇信息，所以很大程度上降低了标注器的准确率，因此人们尝试用词语的其他特征以及上下文来提高对未登录词的词汇概率估计。

------

​	隐马尔可夫模型初始化的方法，主要有两个，分别是：$Jelinek$以及$Kupiec$的方法。$J$的方法是针对词典中的所有词，用$P(w^k|t^i)$的最大似然估计初始化$HMM$，并假设词与它的每个可能的标记同时出现的概率是相等的。

而$K$的方法优点在于不需要为每个词单独地调整参数集合，通过引入**等价类**，总的参数数量大大减少了。关于两个方法，可以在课本226和227页找到相关论述和伪代码。

​	在初始化完成后，我们可以通过**前向-后向算法**(第九章讨论过)进行训练$HMM$。在标注时，马尔可夫模型和隐马尔可夫模型之间的差别在于训练模型，两者训练后的正常结果都应该是一个隐马尔可夫模型。所以，标注时两者没有任何区别，只需要使用$Viterbi$算法即可。

​	下面讨论一下每个模型的运用情况：

​	1. 如果有足够大的训练文本，并且与程序所要处理的文本特别相似，我们就用显马尔可夫模型。

​	2. 如果没有足够的训练文本，或者训练文本和测试文本差别很大，但是至少有一些词汇信息，我们就可以用前向-后向算法多迭代几次。

​	3. 如果根本没有词汇信息时，我们便使用隐马尔可夫模型。

#### 4. 基于转换的标注学习

​	这种基于转换的标注学习的优势在于可以利用更大范围的词汇和语法结构规则。在进行这种方法时，我们需要一个已经标注好的语料库和一个词典作为输入数据。首先用**最常用的标记**来标注训练语料库中的每个词，接下来学习算法构建了一个转换的排序表，它把初始的标注转化为接近正确的标注。

##### 	4.1 转换

​		这种基于转换的标注学习非常有意思，它会人为的提前设置好触发环境和重写规则，当触发环境被触发了，我们就会激活重写规则进行转换。在课本229页表10.7可以看出设定好的触发环境，而具体例子可以看表10.8。

##### 	4.2 学习算法

​		学习算法的思想是，最初我们使用最常见的标记来标注每个词。在每次迭代中，我们选择最可能减少错误率的转换，通过标注过的语料库$C_k$中被错误标注的词语的数目来衡量错误率$E(C_k)$。当没有能够降低超过预先指定阈值$\epsilon$大小的错误率的转换时将停止。算法伪代码可以看课本230页图10.3。

#### 5. 标注准确率和其他

​	在标注中，标注性能主要依赖于下面几个因素，且这些因素比标注方法的选择对于性能有更大的影响：

- **可以获得的训练数据量**。通常越多越好
- **标记集**。通常，标记集越大，潜在歧义就会越多，标注任务就越困难。例如一些标记集中区分了介词to和不定时to，有些却没有。那些没有区分的标记集就不会把to标注错误。
- **训练语料库及词典和应用语料库之间的差别**。如果训练文本和应用文本来自同一个语料源，那么准确率就会很高。否则性能可能会很差。
- **未登录词**。大量未登录词的出现会大大降低标注算法的性能。

​    一些其他的标注方法，比如神经元网络、决策树、基于存储的学习(k近邻)和最大熵模型等，在课本后面可能会介绍到。

​	标注属于$NLP$整个应用中的中间层，基于它可以衍生出很多应用，即它是很多应用的基础。最重要的应用为**部分句法分析**、词汇获取以及**信息抽取**。

​	部分句法分析是指名词短语识别等具体应用，词汇获取在第八章介绍过，信息抽取为找到模板中预定义槽的值，例如天气预报。

------

这一章很需要前面几章的内容作为基础，且词性标注问题在$NLP$中占很大比例。

