### 统计自然语言处理-第六章-统计推理：稀疏数据集上的n元语法模型

​	这一章主要讲了统计推理在语言建模的应用，包括**处理数据、平滑等**等内容，我们这章将以根据当前已知词预测下一个词这个例子进行讲解。之前在[Natural Language Processing with Probabilistic Models](https://github.com/MrSunCodes/-Natural-Language-Processing/blob/main/Natural%20Language%20Processing%20with%20Probabilistic%20Models/03_autocomplete-and-language-models/assignment/C2_W3_Assignment.ipynb)这个Auto-Complete Task中也学习到了相关的知识。我只是跟随整个流程走了一遍，大体懂了n-gram语言模型的一些过程，但是其中的运用基本上都是以最简单的方法进行的，本文也对其中每个环节提出了经典以及更好的替换方法，可以获得更好地结果。

------

<!--more-->

#### 1. Bins: 构造等价类

​	单词预测任务可以看做是估计下面的概率方程P:

​	$P(w_n|w_1,...,w_{n-1})$

​	并且我们需要做一些合理的假设，如**马尔克夫假设**，即只有当前的上下文(前面的很少几个词)可以影响下面要出现的词。如果我们构造一个模型，这个模型的所有历史都是前n-1个已经出现的词，那么我们就有了一个(n-1)阶马尔可夫模型，或者称为n元语法模型(这个n元语法模型的第n个词就是我们要预测的词)。通常我们使用的模型是**bigram和trigram**，由于**unigram**仅仅考虑了词出现的频率，所以在实际进行预测下一个词这个specific task的时候，正确率往往非常低。

​	当然我们在构建**bigram和trigram**语法模型时，参数是一个我们不能忽略的问题。由于参数跟词集大小直接相关，所以我们可以通过**词干化(stemming，删除词语的变性结尾)**的方法来构造等价类，使得很多相同前缀的词可以分成一个等价类，从而大大减少参数，更好地训练。

#### 2. 统计估计

​	有了一定量的训练数据，也有了一些特征等价类别后，我们目标就是对这些数据的目标特征找出一个好的概率估计。对于n元语法模型的例子，我们感兴趣的是:

​	$P(w_n|w_1,...,w_{n-1})=\frac{P(w_1,...,w_{n})}{P(w_1,...,w_{n-1})}$  

> 证明：$ P(A|B) = \frac{P(A,B)}{P(B)}\ $

​	在进一步论证一些方法之前，需要声明一些符号，这些符号在后面的公式和证明中都会用到，如下表：

|       $N$        |                训练实例的数量                |
| :--------------: | :------------------------------------------: |
|       $B$        |               训练实例的类别数               |
|     $W_{1n}$     |      训练文本中的一个n元祖$W_1,...,W_2$      |
| $C(W_1,...,W_2)$ | 训练文本中的一个n元祖$W_1,...,W_2$出现的频率 |
|       $r$        |               一个n元组的频率                |
|      $f()$       |              一个模型的频率估计              |
|      $N_r$       |          含有r个训练实例的类别数目           |
|      $T_r$       |       在更多数据中频率为r的n元组的总数       |
|       $h$        |                 先前词的历史                 |

$B$这里可能有点不太好理解，其实它就是词汇表$V$的大小，就是语料库中出现过的单词种类。

##### 1. 最大似然估计

​	当我们需要根据先前出现过的词来预测下一个词，我们可以使用**相对频率**作为概率估计。比如词串$comes\ across$有十个训练实例，其中8次后面接的是as，一次接more，一次接a，那么相对频率即为：

$P(as)=0.8 \ P(more)=0.1\ P(a)=0.1\ P(\chi)=0.0,\chi为不是前三个词的任意一个$

我们使用的是$P(as)$的概率，这种估计称作极大似然估计(MLE)，之所以称之为最大似然估计，是因为它选择的参数值对于训练数据给出了更高的概率。即

$P_{MLE}(w_1,...,w_n)=\frac{C(w_1,...,w_n)}{N}$

$P_{MLE}(w_n|w_1,...,w_{n-1})=\frac{C(w_1,...,w_n)}{C(w_1,...,w_{n-1})}$

以上两个式子的证明可以看第二章数学基础部分。

###### 	缺陷

​	一般来讲，$MLE$不适合于自然语言处理中的统计推理，这是因为数据稀疏引起的。一般我们使用一个语料库时，即便是非常大的语料库，也很难摆脱数据稀疏的问题。由于对于没有发生过的事情，$MLE$指定了0概率，那么当进行长句子生成或预测时，我们需要将所有的概率全部乘起来得到新的句子的生成概率。但是这就会出现0概率估计的情况，这种情况是我们不愿意遇到的，因为即使句子再语句不通顺或者错误百出，我们也想要有一个极小的概率去生成或预测它。但如果我们从语料库的数据稀疏问题下手，我们又不太可能找到一个语料库把所有单词的排列组合都出现过一遍，所以，**平滑**就显得尤为重要。

##### 2. 平滑

​	这一节会介绍一些平滑的方法，包括拉普拉斯平滑法(Laplace法则)、Lidstone法则(Jeffreys-Perkes法则)。

###### 	2.1 Laplace法则

​		Laplace法则是非常符合直觉的，公式如下：

​		$P_{Lap}(w_1,...,w_n)=\frac{C(w_1,...,w_n)+1}{N+B}$

​		这个处理方法也被称作加1法，注意这里B是类别数量。通过这么处理可以将一小部分概率有效地转移到了未知事件上。不过Laplace法则仍有缺陷，就是对于大量未知的词汇来说，其将大部分概率都转移到了未知事件上，代价就是极大地减少了频繁出现事件的估计概率。即虽然解决了未知事件出现的概率问题，但是却影响了频繁出现事件的概率。

###### 	2.2 Lidstone法则(Jeffreys-Perkes法则)

​		Laplace法则是个很好的开拓者，根据他的思想我们可以有另外一个非常有效的法则，Lidstone法则。该法则思想是分子不是简单地加一，而是加一个较小的正值$\lambda$,公式为：

​		$P_{Lid}(w_1,...,w_n)=\frac{C(w_1,...,w_n)+\lambda}{N+B\lambda}$

​		进一步，当我们设$\mu=\frac{N}{N+B\lambda}$时，仅需要带入上述公式，就可以进一步改写公式为：

​		$P_{Lid}(w_1,...,w_n)=\mu\frac{C(w_1,...,w_n)}{N}+(1-\mu)\frac{1}{B}$

​		最常用的$\lambda$的值是1/2，这个法则又称为Jeffreys-Perkes法则,或者称为期望似然估计($Expected\ Likelihood\ Estimation,ELE$)

​		这个方法是非常有用的，我们可以使用一个小的$\lambda$来避开将太多的概率空间转移到未知事件上，有效地填补了laplace法则的缺点。

##### 3.交叉验证

​	交叉验证的思想是，训练数据的每一部分既作为最初的训练数据也作为留存数据。在介绍交叉验证思想之前，需要介绍一些其他方法作为验证手段。

###### 	3.1 划分训练集和测试集

​		这个方法是最原始的方法，一般我们可以通过8:2的比例将数据集划分为训练集和测试集，这样做的缺点在于，模型的训练程度很大一部分在于你的划分方法，甚至带点运气在里面。并且如果你将测试集划分出去，那么你将会天然地少了测试集作为训练数据集，那么模型的效果就会降低。

###### 	3.2 LOOCV(Leaving-One-Out **Cross Validation**)

​		这个方法的思路是只讲一个数据作为测试集，其他数据都作为训练集，并且重复n次，n为数据集的大小。即在n次循环中，每次将n-1的数据量作为训练，剩下一个数据进行测试，结果就是我们训练了n个模型，最终的MSE就是n个MSE的算术平均。

​		这个方法比第一个方法的好处在于没有划分的要求，并且所有数据都参与过训练，充分利用数据集。但是缺点也很明显，训练时间会变得非常长，因为相当于训练了n个模型。

###### 	3.3 K-fold **Cross Validation**

​		这个方法是第二个方法的改进，即每次不选取一个数据而是k个数据，例如k=m，那么我们将数据集分成m份，每次取其中一份作为测试集，m-1份作为训练集，最后相当于训练了m个模型，最终的MSE就是m个MSE的算术平均。这个方法的好处是，训练时间相对于LOOCV少了很多，因为$k<<n$。

##### 4.Good-Turing估计

​	Good-Turing估计是在统计中相信可靠的统计数据，而对不可信的统计数据打折扣的一种概率估计方法。是一种重新估算概率的公式，也可以视为平滑算法。主要思想将非零N元语法的概率均匀分给一些低概率语法，以修改最大似然估计与真实概率之间的偏离。

​	这个方法讲解具体可以看我搬运的[带有英文字幕的B站视频](https://www.bilibili.com/video/bv1Dv411w7FN)。

​	Good-Turing估计的思想是：当遇到一个我们没有碰到过的事件时，我们就用**仅出现过一次的事件**去作为它的最大似然估计。而对于碰到过的事件，我们用**其本身和比其本身出现次数多一次的事件**去抵扣它的最大似然估计。具体可以看如下公式：

​	$P_{GT}^*(Things\ with\ zero\ frequency)=\frac{N_1}{N}$

​	$P_{GT}^*(Things\ with\ c\ frequency)=c^*/N=\frac{(c+1)N_{c+1}}{N_cN}$

​	这个方法充分估计了相关词的影响，并且给出了相当漂亮的平滑曲线。

##### 5.折扣方法

​	本节主要讲了折扣的方法，思想是将所有的非零$MLE$频率分配到未知事件上，很类似于平滑思路。讲解的方法有**绝对折扣以及线性折扣**。

- 绝对折扣。令$C(w_1,...,w_n)=r$,则

  $P_{abs}(w_1,...,w_n)=(r-\delta)/N\ (r>0)$

  $P_{abs}(w_1,...,w_n)=\frac{(B-N_0)\delta}{N_0N} (其他)$

  其中$N_0$是含有0个训练实例的类别数目，没有在语料库中出现过的类别数目。

  在绝对折扣中，将所有的非零$MLE$频率用一个小的常数$\delta$折扣，并将频率增益平均分配到未知事件上。

- 线性折扣。

  令$C(w_1,...,w_n)=r$,则

  $P_{lin}(w_1,...,w_n)=(1-\alpha)r/N\ (r>0)$

  $P_{lin}(w_1,...,w_n)=\alpha/N_0\  (其他)$

  在线性折扣中，非零的$MLE$频率乘以一个稍微小于1的常量，留出的概率仍分配到未知事件上。

​    这两个估计方法同平滑思路是一样的，都是用一些小数字$\epsilon$代替未知事件的零概率，并且重新调整其他的概率，使得概率总和为1，在调整方法中选择减一个常量或者乘一个常量，对概率进行折扣。

##### 6.组合估计法

​	平滑和折扣方法还有一个小的缺陷，就是对于所有没有出现过的或者很少出现的n-gram，给定相同的概率。但给定相同概率并不是一个好方法，我们希望通过n-gram中的(n-1)gram来产生一个更好的概率估计。如果(n-1)gram本身很少出现，那么我们就给n-gram一个低的估计值，否则，我们可以适当给一个高的估计值。这个思想就是希望考虑n-gram的历史，而不是仅仅根据其没有出现过进行平滑，从而使得出现次数一致的词总是有同样的概率，抛弃了词本身的性质。

###### 	6.1 简单的线性插值

​		举个例子，对于解决trigram模型中的数据稀疏问题，我们可以将bigram模型和unigram模型组合到trigram模型中，将它们线性组合起来，对每个概率分布**加权**即可得到新的概率方程。做法是：

​		$P_{li}(w_n|w_{n-2},w_{n-1})=\lambda_1 P_1(w_n)+ \lambda_2 P_2(w_n|w_{n-1})+\lambda_3 P_3(w_n|w_{n-1},w_{n-2})$

​		其中$0\leq \lambda_i \leq 1$且$\sum_i\lambda_i=1$,设定权值$\lambda_i$也是一个挑战，我们可以通过EM算法自动完成此事。EM算法之后会介绍。

###### 	6.2 Katz回退算法

​		这里不再详细介绍。

###### 	6.3 一般化线性插值

​		一般化线性插值，相比于简单地线性插值，其提供了一个更加具有泛化能力以及一般化且有效的模型，它的权值不再是常数而是一个关于历史的函数。做法是:

​		$P_{li}(w|h)=\sum ^k \limits_{i=1} \lambda_i(h)P_i(w|h)$

​		其中在公式里，任取h，都有$0\leq \lambda_i(h) \leq 1$且$\sum_i\lambda_i(h)=1$

##### 7.结论

​	本章相比于前面几章难度比较大，公式也比较多，需要我们好好地仔细查看，反复地看，才能有所收获。这一章针对的问题是统计NLP中非常典型的问题：预测下一个词或者句子。作者最后也给出了它的结论：即使用**Good-Turing估计**和**线性插值法(或回退算法)**可以避开数据稀疏的问题，并且在语料库中有着很好的效果。

