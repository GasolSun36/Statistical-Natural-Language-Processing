### 统计自然语言处理-第十五章-信息检索

​	第十五章主要内容是信息检索。信息检索这个概念和方法我在$Chengxiang\ Zhai$老师的$Text\ Retrieval$中有具体学过。具体可以看我的$github\ repo.$[https://github.com/MrSunCodes/Data-Mining-UIUC/tree/main/Text%20Retrieval](https://github.com/MrSunCodes/Data-Mining-UIUC/tree/main/Text%20Retrieval)。其中，我还做了课程的$Assignment$，是一个基于$C++$语言写的自然语言处理包$meTA$，运行在$Linux$虚拟机上：[https://mrsuncodes.github.io/2021/08/12/text-retrieval-Assignment/](https://mrsuncodes.github.io/2021/08/12/text-retrieval-Assignment/)

------

<!--more-->

#### 1.  基本概念和背景

​	信息检索的传统问题是用户输入一个待查询信息，系统回复一个包含所需信息的文档列表。这种回复有两种类型：**精确匹配**和**模糊匹配**。精确匹配是指返回信息和检索信息之间的精确匹配，是商业信息系统中的广泛应用。而模糊匹配，是返回的和查询信息相关度大的文档，相关度越大返回优先级越高。

​	在信息检索的子领域中，包括了很多问题例如最经典的文本分类问题。其中，**过滤**和**信息路由**是文本分类的两个特例应用。信息路由是指输出结果按照估计相关度降序排序，典型应用就是邮件分发系统；而过滤是指将相关度小于某个阈值的文档过滤掉，相关度用概率值的形式表示。

​	大多数信息检索系统都要为数据集创建**倒排索引**。倒排索引是指其列出每个单词出现过的所有的文档，以及每篇文档中出现的频次，建立倒排索引可以大大方便查询字段的查找，因为只要在索引中找到该单词，就可以确认它的出现文档。当然，倒排索引可以进一步优化，优化的方法是加入单词的**位置信息**，下面是优化的例子：

> 例如用户输入查询字段$car\ insurance$，我们可以在倒排文档中同时查找包含$car$和$insurance$的文档，首先将两个文档集进行交集运算，得到它们同时出现的文档集合，然后通过单词的位置信息，确定保留那些$car$出现在$insurance$之前的文档，即需要的结果。这种方法显然要比处理所有文档的方法效率高得多。

​	当然，在创建倒排索引时，不能所有单词都创建，我们需要去掉停用词表(例如$where,who,to,a,the$)等等，这些词在文档中出现的次数很频繁，但是却对文章主体没有什么贡献，排除这些词后可以大大减少索引的大小。当然除了停用词表，还有一个就是词干化$stemming$，一般来说采用截断法，即去掉词缀中单词的一部分。当然，这些方法在一些$python$的自然语言处理包(比如$nltk$)已经实现了，只需调用函数即可。

​	在对返回检索信息的评估中，我们主要运用**准确率和召回率**两个指标进行评估。这两个指标已经讲过多次，这里就不再细说了。当然这里还是要提一下将两个指标融合在一起的$F$测量，即：

​	$F=\frac{1}{\alpha^{\frac{1}{P}}+(1-\alpha)^{\frac{1}{P}}},其中P是准确率，R是召回率，\alpha是两者的权重因子。$

​	在返回的结果中，我们一般使用的是文档概率排序准则($PRP$)，这是将文档按照其相关概率的降序排列进行返回，是最优的文档排序方法。这个方法有两个基本假设，即一是集合中文档之间是不相关的，二是复杂的初始查询字段可分解为单独的子查询，并且各自优化再返回归并概率。

#### 2. 向量空间模型

​	向量空间模型是广泛应用于信息检索的模型，主要思想是用高维表示查询和文档，比如空间中的每一维都可以是文档中的一个单词。我们可以通过找到文档向量和查询向量之间的距离去关联它们的相似度。一般相似度可以用余弦来表示，即：

​	$cos(\vec{q},\vec{d})=\frac{\sum^n_{i=1}q_id_i}{\sqrt{\sum^n_{i=1}q^2_i} \sqrt{\sum^n_{i=1}d^2_i}},其中\vec{q}和\vec{d}都是n维空间上的向量$

​	那么我们如何来计算向量空间当中词条的权重呢，最简单的方法是统计词条在文档中出现的次数，当然，更好的方法是考虑下面三个重要信息：**词条频度**、**文档频度**和**收集频度**。如下定义：

|   度量   |  符号标识  | 定义                           |
| :------: | :--------: | ------------------------------ |
| 词条频度 | $tf_{i,j}$ | $单词W_i在d_j文档中的出现次数$ |
| 文档频度 |   $df_i$   | $出现单词W_i的文档数$          |
| 收集频度 |   $cf_i$   | $单词W_i出现的总次数$          |

​	其中，$df_i \le cf_i,\sum_jtf_{i,j}=cf_i$

​	词条频度描述了词条在给定文档中的重要程度，值越大说明该词对文档的描述程度越高，更能准确反映文章内容。

​	文档频度表示词条的**信息度**，如果一个词在每个文档中都有可能出现，很显然说明这个词和文档内容的相关度不大；否则说明相关度很大。

​	收集频度就是单词出现的总次数，作为一个数据。

​	当我们把词条频度和文档频度关联在一个公式中，则：
$$
weight(i,j)=
\begin{cases}
(1+log(tf_{i,j}))log\frac{N}{df_i}, & 若tf_{i,j} \ge 1  \\
0  , & 若tf_{i,j} = 0 \\
\end{cases}
$$
​	其中$N$是文档集中的总文档数，$log\frac{N}{df_i}$很关键，当$df_i$为1时，这个值最大，当$df_i$为$N$时，这个值最小，即词出现的文档数越多，权重越小，成反比。

#### 3. 词条分布模型

​	计算词条权重的另一个思路是构建一个词条分布的概率模型。本章介绍的方法是三种概率模型，即泊松分布、二重泊松分布、k重混合分布。

##### 	3.1 泊松分布

​		泊松分布是统计学中一个非常重要的分布，描述了特定事件在一个固定大小的空间中的分布情况，定义如下：

​		$p(k;\lambda_i)=e^{-\lambda i}\frac{\lambda^k_i}{k!}$

​		在泊松分布中，样本期望值和方差都是$\lambda_i$，$\lambda_i$是单词$w_i$在一篇文档中的平均出现次数，等于$\lambda_i=\frac{cf_i}{N}，cf_i是单词w_j出现的次数，N是总文档数$。

​		当然，使用泊松分布需要满足三个条件：

​		1. 在短篇幅文档中词条出现一次的概率和该篇文档的长度成正比。

​		2. 词条出现一次以上的概率远远小于词条出现一次的概率。

​		3. 事件的出现在文档各部分相互独立。

##### 	3.2 二重泊松分布

​		二重泊松分布比单泊松分布拟合实词分布模型效果更好，是两个泊松分布的混合。该模型将文档分为两类，第一类文档对应于该词条出现频度较低的文档，第二类对应于出现频度较高的文档，公式为：

​		$tp(k;\pi,\lambda_1,\lambda_2)=\pi^{-\lambda_1}\frac{\lambda_1^k}{k!}+(1-\pi)e^{-\lambda_2}\frac{\lambda_2^k}{k!}$

​		其中，$\pi$是文档在第二类文档中出现的概率，$(1-\pi)$对应于文档在第一类文档中出现的概率，$\lambda_1和\lambda_2$为词条在第一类和第二类中出现次数的平均值。

##### 	3.3 k重混合分布

​		k重混合分布比前两种分布效果更好，公式为：

​		$P_i(k)=(1-\alpha)\delta_{k,0}+\frac{\lambda}{\beta+1}(\frac{\beta}{\beta+1})^k$

​		其中：
$$
\delta_{k,0}=
\begin{cases}
1, &k=0\\
0, &其他
\end{cases}
$$
​		参数$\frac{\beta}{\beta+1}=\frac{cf-df}{cf}$实际上决定了$\frac{P_i(k)}{P_i(k+1)}$的大小。例如这个比值为0.1,那么出现该词条一次的文档数是出现两次文档数的10倍。($P_i(k)表示w_i在一篇文档中出现k次的概率$)

#### 4. 潜在语义索引

​	书中讲的潜在语义索引实际上是一种降维方式，运用了奇异值分解的想法，又顺便讲了一下线性回归的最小平方和法(最小二乘法)。最小平方和法的公式以及两个求偏导建议推导一下，在课本345和346页，这里不再赘述，但是例子非常好，可以更好地理解线性回归。

​	奇异值分解，我直接参考的[https://www.bilibili.com/video/BV16A411T7zX?from=search&seid=14525895075340579980&spm_id_from=333.337.0.0](https://www.bilibili.com/video/BV16A411T7zX?from=search&seid=14525895075340579980&spm_id_from=333.337.0.0)这个视频，学长讲的非常好，可以直接用来理解奇异值分解的思想，当然还有PCA，可以一起学习：[https://www.bilibili.com/video/BV1E5411E71z/?spm_id_from=333.788.recommend_more_video.-1](https://www.bilibili.com/video/BV1E5411E71z/?spm_id_from=333.788.recommend_more_video.-1)。两者都是降维的方法，且有共性。

​	在自然语言处理中，运用潜在语义索引可能会有计算复杂度很大的缺陷，这里我们可以做的是：对于非常大的文档集合，选取它们的子集和出现频率很高的词汇是降低计算量的一个有效方法。

#### 5. 篇章分割

​	篇章分割的主要任务是对**文档结构**进行分析，返回查询结果是段落而不是整篇文档。而事先篇章分割的算法主要是$TextTiling$算法。

​	算法的思想是在一篇文档中寻找从一个主题转到另一个主题的**过渡部分**。主要由三个部分组成：**紧凑度计算**、**深度计算**和**边界选择**。

​	1. 紧凑度计算主要衡量间隔点的质量。我们希望找到紧凑度低的间隔点，紧凑度低意味着前后的连续性差，更适合在该间隔点进行分割。

​	2. 深度计算作用是比较某一个间隔点的周围间隔点之间的紧凑度，值越低深度值越大。

​	3. 边界选择部分是通过比较深度值来确定切割的候选点。

​	计算紧凑度的方法主要有三种：**向量空间计分法**、**文本块比较法**和**新词引入法**。第一种方法是确定间隔点并将其左右相邻子句表示成向量，计算向量之间的相关系数，该相关系数是同切割点的概率成反比的，相关系数越大，说明前后的紧密程度越高。文本块比较法是在第一个方法上只利用文本块内词条的出现频度而不利用文档频度。新词引入法计算紧凑度的值是同文本块中的新词个数成反比。

​	实验说明，文本块比较法在上述三个方法中效果最好。

​	在第二步将紧凑度转化为深度值时，具体方法就是将当前间隔点和左右相邻点的**高度差相加**作为深度值。第三步是边界选择器，它估计深度值的均值和方差，然后选择所有深度值高于$\mu-c\sigma$的间隔作为边界，其中$c$为常数。

------

​	这一章的主要内容是信息检索的一些具体流程、模型和处理方法，给了我们一个基础的概念理解。
