

### 统计自然语言处理-第七章-语义消岐

​	第七章主要内容是语义消岐，包含的方法主要有有监督的语义消岐，基于词典的语义消岐以及无监督的语义消岐，其中有些算法比较难理解，需要多读书且查阅课外资料进行更好的理解。

------

<!--more-->

​	什么是语义消岐？语义消岐的任务是确定一个歧义词的哪一种语义在一个特殊的使用环境中被调用，通过考虑词汇的上下文可以确定其具体的语义。大部分的词性标注模型简单地使用当前上下文，而语义消岐模型通常试图使用规模广泛一些的上下文中的实词。

#### 1.预备知识

##### 	1.1 有监督学习和无监督学习

​			这个概念非常简单，不再赘述，是机器学习中的基本问题。

##### 	1.2 伪词

​			由于当我们测试消岐算法的结果时，往往不太好找数据集，人工标注数据太费事费力。在测试数据很难得到的情况下，通常我们会方便地产生一些人工数据，这些数据称为伪词。

​			伪词的方法是合并两个或多个自然词汇，比如我们可以创建伪词banana-door，然后在语料库中用该词去替换所有的banana和door。此时，带有伪词的文本作为歧义源文本，最初的文本作为消岐后的文本。

##### 	1.3 算法性能的上下界

​			算法性能的上下界也非常好界定。上界我们可以界定为人工的效能，因为一般人工处理消除歧义问题是非常有效的。下界可以界定为词汇使用最频繁的语义来作为结果测试后的效能，这个方法非常简单且实用。

##### 	1.4 一些用到的符号表达

|         符号         |        符号表示含义        |
| :------------------: | :------------------------: |
|         $W$          |         一个歧义词         |
| $s_1,...,s_k,...s_K$ |     歧义词$W$的语义项      |
| $c_1,...,c_i,...c_l$ |   $W$在语料库中的上下文    |
| $v_1,...,v_j,...v_J$ | 消岐使用的上下文特征(单词) |

其中上下文可以理解为该单词前后左右的一些单词。

#### 2.有监督消岐

​	在本节中，主要介绍了贝叶斯分类以及基于信息论的方法。其中贝叶斯分类是把上下文看做一个无结构词集，整合了上下文窗口中众多的词汇信息。而基于信息论的方法仅仅考虑了上下文中的一个信息特征，这个信息特征可以很灵敏地反映上下文的结构。

##### 	2.1 贝叶斯分类

​		贝叶斯分类原理在于贝叶斯公式，它对于每个独立的例子，选择带有最高条件概率的类，因此有最小的概率。但是它基于两个假设，一是上下文中所有的结构和词语顺序都可以被忽略。二是在可有重复的单词集中出现的词独立于其他词。这两个假设或推论显然是不对的，但是分类器的决策仍然是最优的，导致其效果出人意料的好。

​		其伪代码在课本148页，这里主要讲一下大体思路：

​		因为我们需要通过贝叶斯进行计算，目标函数为

​		$s^{'}=argmax_{s_k}[logP(c|s_k)+logP(s_k)]$

​		这样写其实就是把贝叶斯公式去除了分母(分母为先验概率即常数，在找最大值时可以略去)，然后通过添加$log$将乘法转变为加法，并取最大值即可。(具体推导可以查看第二章-数学基础)

​		且公式中改写了目标函数，为:

​		$score(s_k)=logP(s_k)$

​		$score(s_k)=score(s_k)+logP(v_j|s_k)$

​		其中具体式子由下面公式计算：

​		$P(v_j|s_k)=\frac{C(v_j,s_k)}{\sum_iC(v_i,s_k)}$，$P(s_k)=\frac{C(s_k)}{C(w)}$

​		上述公式中$C(v_j,s_k)$是训练语料中的$v_j$在语义$s_k$的上下文中出现的次数，$C(s_k)$是$s_k$在训练语料中的出现次数，$C(w)$是歧义词$w$出现的总次数。关于贝叶斯分类在消岐中的应用，可以参考我们学校苏劲松老师的一篇论文：《贝叶斯分类在词义消歧中的分析》。可以通过网上下载，我也会放到后续下载链接中。

##### 	2.2 一个基于信息论的方法

​		信息论我们在第二章数学基础已经讲过，在这里介绍使用信息论中的互信息为基础的Flip-Flop算法，该算法的每一步迭代都必须满足使互信息$I(P;Q)$单调增加，所以算法一个很自然的终止条件就是互信息不再增加或增加很少。

​		假设我们想要根据宾语翻译$prendre$且我们已经有了${t_1,...,t_m}={take,make,rise,speak}$和${x_1,..,x_n}={mesure,note,exemple,decision,parole}$，**算法的主要思想是**：先随机划分$P$,这里$P$为语义划分，划分第一语言(即$t_i$)，$Q$为指示器划分，划分第二语言(即$x_i$)。根据$P$来划分$Q$,使得划分后$I(P,Q)$增大，然后根据划分后的$Q$来调整$P$,再次使得$I(P,Q)$增大，不断重复这个过程，直到我们之前讨论过的终止条件。互信息的结算公式可以查看第二章-数学基础部分。课本150页的例子也很好的说明了这点，可以结合例子更好的理解这个算法。

​		当算法结束，划分完成后，对于每个出现的歧义词，我们要确定它的指示器值$x_i$，如果$x_i$在$Q_1$中，那么歧义词的语义为语义1；否则会为语义2。当然这个算法只能处理两个语义的歧义情况，多个语义的歧义情况便是这个算法的拓展版本，书中没有描述，这里也就不再记录了。

#### 3. 基于词典的消岐

​		基于词典的消岐是给定消岐的句子，以及消岐对象在字典中的解释(这个词典可以是任何词典)，然后通过词典来进行语义消岐。

##### 		3.1 基于语义定义的消岐

​			这里介绍的算法是非常常用且经典的**Lesk算法**，他认为一个词在词典中的词义解释与该词所在句子具有相似性。这种相似性可以由相同单词的个数来表示，比如“cone”和"pine"的意思分别为：

```
CONE
1. solid body which narrows to a point
2. something of this shape whether solid or hollow
3. fruit of certain evergreen trees
```

```
PINE 
1. kinds of evergreen tree with needle-shaped leaves
2. waste away through sorrow or illness
```

​			其中，Pine #1和Cone #3的重合度为2，这个重合度最大，从而表示这两种意思具有最大相似性。

​			这个方法非常简单且符合直觉，但是也存在不足，即一般主题范畴并不适合于特殊领域。比如一个词有两种解释，哺乳动物以及电子器件，而计算机领域文章中这个词很少会出现哺乳动物这一范畴。针对这一问题，有一个更好的解决方法，是由$Yarowsky$设计的，其增加了对语料库的主题分类。其可以将语料库中$t_i$类上下文中频繁出现的词加到$t_i$类里，提高算法效率以及准确性。这里不再过多赘述，可以看课本154页例子以及伪代码进行理解。

##### 		3.2 在第二语言语料库翻译基础上的消岐

​			需要消岐的语言称为第一语言，在双语词典中的目标语言称为第二语言。具体可以看一下155页课本上面的例子，浅显易懂。这个算法的思想是通过查看第二语言语料库中某个词的两种语义的翻译和它的固定搭配出现的次数作对比，次数多的那个翻译就是它在第二语言中的意思。当然，这个算法的缺点在于，如果两个次数分别是10次和5次，那么错误翻译的可能性会非常大(5/5+10=0.33)，在很多情况下，不做结论比得到一个错误概率较大的结论更好。所以Dagan和Itai他俩拓展的算法中，设定了置信度，只有当置信度大于等于90%时才会给出决定。

##### 		3.3 每篇文本一个语义，每个搭配一个语义

​			 $Yarowsky$曾提出过两个约束：

​			3.3.1 **每篇文本一个语义**。在任意给定的文本中目标词语义有很强的一致性。

​			3.3.2 每个搭配一个语义。根据和目标词之间的相对距离，次序和句法关系，相邻词提供了可用于判断目标词语义的很多线索信息。

​			这两个约束在小文本语料库中非常实用，具体就是考虑一个单词plant，如果plant第一次出现使用的语义是living being，那么它后面的出现也非常有可能使用语义living being，即**每篇文本一个语义**这一约束，

​			$Yarowsky$混合两个约束设计了一个算法，这个算法的思想是：

​			首先确定每个歧义词和它出现的固定搭配，比如$fish\ within\ window$,即在一个窗口也就是$bass$的上下文中，出现了$fish$，那么$bass$大概率就是鱼的意思，而$striped\ bass$，同理也是鱼的意思，$guiter\ within\ window$，那么$bass$就是乐器贝斯的意思，$bass\ player$中的$bass$也是乐器贝斯的意思。当我们在词典中有了这么一个固定搭配的时候，我们再处理歧义变为非常好解决。具体伪代码还有例子可以看课本157页，也可以看我bibilibili转载的视频，都有讲解可以更好的理解。[bilibili视频](https://www.bilibili.com/video/BV1Qh411p76C?spm_id_from=333.999.0.0)

#### 4. 无监督消岐

​		无监督消岐的意思是不依靠词典以及人为给的标签，通过聚类的方法来辨别出它们。因为在无监督消岐中参数估计不是根据有标注的训练数据，所以我们一开始要随机初始化参数$P(v_j|s_k)$，并根据EM算法(本书后面会讲到)进行重新估计$P(v_j|s_k)$,使得模型给定数据的似然值最大。EM算法要保证每一步中模型似然对数的值是增加的，因此这个算法的终止条件也是似然值不再明显增加。

​		算法是在人为给定的语义数目$K$内运行算法，语义越多，模型结果越复杂。但是，我们可以考察每个新语义加入后模型似然值增加的幅度，以确定是否可以加入新语义。如果似然对数值增加幅度很明显，表明新语义反映了数据的一个主要部分，那么这个新语义数目就是可取的。如果只是一般性地增加，那么新语义仅仅反映了随机变化，其是不可取的。当然，由于随机初始化参数值，那么其造成的结果也就不一样，所以需要多次试验取平均值。

#### 5. 总结

​		最后，课本总结了什么是语义这个问题以及现在最常用的消岐方法是基于词典的消岐，并且很多歧义词服从的是偏斜分布,所谓的偏斜分布是指一个语义被一个词的大多数情况所使用，且偏斜分布是歧义词的典型分布。

