### 统计自然语言处理-第十一章-概率上下文无关文法

​	第十一章主要内容是讲了概率上下文无关文法这一模型--$PCFG$($(Probabilistic\ Context\ Free\  Grammar)$)。以及运用这个模型如何进行计算词性标注的概率。因为书上写的内容非常抽象，所以这一章我主要是以哥伦比亚大学[Natural Language Processing](https://www.youtube.com/watch?v=dH2dHMT0tOA&list=PLlQBy7xY8mbKypSJe_AjVtCuXXsdODiDi&index=6)这门课作为学习资料进行学习，而记录的内容也跟这门课的内容有关。

------

<!--more-->

​	概率上下文无关文法是以前面第三章和第八章讲的内容为基础，都是讲了一个句子可以解析成很多棵语法树，而不同语法树各自的概率都不相同，如何高效快速的计算生成语法树的概率，是概率上下文无关文法的话题。

#### 1. $PCFG$介绍

​	$PCFG$与之前我们讲到过的语法树最大的不同就是它多了概率，即我们可以计算每个语法树生成的概率，从而进行判断哪棵语法树是最有可能生成的。即：

​	$p(t)=\prod^n_{i=1}q(\alpha_i \to \beta_i),\ q(\alpha_i \to \beta_i)是规则\alpha_i \to \beta_i的概率$

​	图片1

​	通过对照表1的一些概率，我们可以使用以上公式去计算这个具体语法树的生成概率，即

​	图片2

​	所以：

​	$p(the\ dog\ laughs)=1.0 \times 0.3 \times 1.0 \times 0.1 \times 0.4 \times 0.5	=0.006$

------

​	那么，我们如何知道表1里面的一些概率呢，当然是用统计学的方法，使用一个大型语料库进行训练，使用最大似然估计，训练的公式为：

​	$q_{ML}(\alpha \to \beta)=\frac{Count(\alpha \to \beta)}{Count(\alpha)}$

​	举个例子，$VP \to Vt\ NP =0.6$，那么就是$p(VP \to Vt\ NP)=\frac{Count(VP \to Vt\ NP)}{Count(VP)}$。而计数我们可以很方便的从语料库中统计出来。

#### 2. 基于动态规划的$PCFG$计算方法

​	对于如何计算一个句子所有可能的语法树，并通过语法树来依次计算所有语法树的生成概率并从中挑选出其中一个最大值，我们用暴力的方法时间复杂度是指数增长的，这里提出一个基于动态规划的方法，可以很好的降低时间复杂度，从而进行高效的计算。

​	我们最终目的是为了找到$arg\max\limits_{t \in \varGamma(s)}p(t)$，即我们最大概率的语法树的下标。

​	我们定义了一个语法树的新的格式，$Chomskey\ Normal\ Form$。这个形式写为：

​	$G=(N,\Sigma,R,S)$，其中：

|   符号   |                       符号表示含义                       |
| :------: | :------------------------------------------------------: |
|   $N$    | 非终结符，非叶子结点的符号集合，即语法树中中间结点的符号 |
| $\Sigma$ |   终结符，叶子结点的符号集合，即语法树中叶子结点的符号   |
|   $R$    |                         规则集合                         |
|   $S$    |                      句子的开始符号                      |

​	关于规则集合，这里有两个必须要遵循的形式:

​	1. $X \to Y_1 Y_2,for\ X \in N,and\ Y_1,Y_2 \in N $，即每个结点最多的出度为2，这样规定语法树只能是二叉树。

​	2. $X \to Y\ for\ X\ \in\ N,and\ Y\ \in\ \Sigma$，即只有中间结点才能推叶子结点，不能反过来。

​	第二个规则好理解，第一个规则举个简单的例子，加入有个规则是$VP \to Vt\ NP\ PP(0.2)$ ，在下一步时我们需要将它写成$VP \to Vt-NP\ PP(0.2),Vt-NP \to Vt\ NP(1.0)$，这样就可以满足规则一二了。

​	因为用到了动态规划的思想，我们这里创建一个动态规划表：

​	$\pi[i,j,X]$，意思是从第$i$个词到第$j$个词，且成分为非叶子结点的概率。我们的目标就是通过动态规划的方法算出$\max_{t \in \varGamma(s)}p(t)=\pi[1,n,S]$。举个例子，下面这样一段话：

​	$the\ dog\ saw\ the\ man\ with\ the\ telescope.$

​	$the$的下标为1，$telescope$的下标是8，那么，$saw\ the\ man\ with\ the\ telescope$就可以用$\pi[3,8,VP]$来表示。

​	再然后，我们需要规定一下算法的一些小细节：

​	1. $\pi[i,i,X]=q(X \to w_i),q(X \to w_i)=0如果X \to w_i不在统计语料库中$

​	2. $\pi[i,j,X]=\max \limits_{(X \to Y Z) \in R \atop s \in \{ i...(j-1)\}}(q(X \to YZ) \times \pi[i,s,Y]) \times \pi[s+1,j,Z]$

​    这个公式需要注意的地方是，首先$max$下面的下表中，$s$只能取到$j-1$，是因为后面有$s+1$。再者，后面是一个标准的动态规划的形式，$i \to s,s+1 \to j$。

​	举个例子，还是以上面的那句话：

​	$the\ dog\ saw\ the\ man\ with\ the\ telescope.$

​	由上面公式我们可得：

​	$\pi[3,8,VP]=\max \{q(VP \to Vt\ Np) \times \pi[3,3,Vt] \times \pi[4,8,NP]$

​	$q(VP \to Vt\ Np) \times \pi[3,4,Vt] \times \pi[5,8,NP]$

​	$...$

​	$q(VP \to Vt\ Np) \times \pi[3,7,Vt] \times \pi[8,8,NP]$

​	$q(VP \to Vt\ Np) \times \pi[3,3,VP] \times \pi[4,8,PP]$

​	$...$

​	$q(VP \to Vt\ Np) \times \pi[3,7,VP] \times \pi[8,8,PP]\}$

​	为什么这里分成了两种方案，是因为:

​	$VP \to Vt\ NP\ 为0.4,VP \to VP\ PP\ 为0.6,且S \in \{3,4,5,6,7\}$

------

有了上面的铺垫，后面的伪代码就非常简单了：

​	$Input:\ a\ sentence\ s=x_1...x_n,a\ PCFG\ G=(N,\Sigma,S,R,q)$

​	$Initialization:$

​	$For\ all\ i\in\{1...n\},for\ all\ X\in N,$

$$
\pi[i,i,X]=
\begin{cases}
q(X\to x_i), & if\ X \to x_i \in R  \\
0, & \text{otherwise}
\end{cases}
$$

​	$Algorithm:$

​	$For \ l=1...(n-1)$

​	$\qquad For\ i=1...(n-l)$

​	$\qquad \qquad set\ j=i+l$

​	$\qquad \qquad for\ all\ X\in N,calculate$

$\qquad\qquad \qquad\pi[i,j,X]=\pi[i,j,X]=\max \limits_{(X \to Y Z) \in R \atop s \in \{ i...(j-1)\}}(q(X \to YZ) \times \pi[i,s,Y]) \times \pi[s+1,j,Z]$

​		$\qquad\qquad\qquad bp(i,j,X)=arg\max \limits_{(X \to Y Z) \in R \atop s \in \{ i...(j-1)\}}(q(X \to YZ) \times \pi[i,s,Y]) \times \pi[s+1,j,Z]$

------

这个算法比较直观，可以直接根据伪代码看一下他的思想，并且有前面的铺垫做背景，可以更好的理解。

#### 3. $PCFG$缺点

​	每个算法都存在缺点，$PCFG$也不例外，与类似$HMM$的线性模型比较，它的学习算法速度很慢，每个句子训练中的迭代都是$O(m^3n^2)$，$m$是句子长度，$n$是语法中非终结符号的数目。

​	其次，这个算法和梯度下降很像，对初始化参数非常敏感，且有局部极值。
