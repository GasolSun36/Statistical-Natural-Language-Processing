### 统计自然语言处理-第十六章-文本分类

​	第十六章主要内容是文本分类，其中用到了很多机器学习的方法，例如决策树、最大熵模型、感知机以及k近邻模型，这些模型都可以进一步去李航老师的统计学习方法中去进一步学习。可以参考我的博客：[https://mrsuncodes.github.io/](https://mrsuncodes.github.io/)该章也是本书的最后一章。

------

<!--more-->

#### 1.  基本概念

​	文本分类是$NLP$中一个重要应用领域。我们之间讨论过的词性标记、语义消岐、介词短语附着、作者识别、语音识别等等都属于分类问题。文本分类问题是把文档按照它们的主题或者主旨来划分类别，这个类别是已经提前分好的，并且划分时可能会将一篇文档划分为一个主题，也可能将其划分为多个主题。

​	文本分类器在统计自然语言处理中有四种方法：决策树模型、最大熵模型、感知机模型以及k近邻模型。而评估方法依旧是准确率和召回率，当然，为了统一准确率和召回率，我们继续使用$F$测量。

#### 2. 决策树模型

​	决策树在李航老师的统计学习方法第五章中有介绍，本书给出的是其在自然语言处理中的应用。决策树学习是将一组无次序无规则的事例中推理出决策树形式的分类规则。其采用**自顶向下**的递归方式，在决策树的内部节点进行属性值比较，并根据比较结果进一步划分到下面的分支，最终到达叶节点并获得分类结果。决策树的一个例子如下图所示：

​	图片1

​	我们可以看到，每个分支都有判断的条件，我们可以根据某个数据的特征进行划分，从根节点一路走到叶节点，并获得最终的分类结果。在建立决策树之前，我们还需要建立**数据表示模型**。为了简化问题，我们可以直接用向量空间模型作为数据表示，即用向量来表示文本，以词作为特征项。这种方式很类似于$one-hot$表示$word$，但是0和1用来表示文档中是否出现过这个词不太清楚，更进一步我们使用词频来代替0,1。词频又分为绝对词频和相对词频，绝对词频是指词在文本中出现的频率表示文本， 相对词频为归一化的词频。

​	举个例子，如果我们在训练集中运用卡方分布计算某个类别中单词的得分，再取top20作为文档的代表词即特征。那么词条权重计算公式为：

​	$s_{i,j}=round(10 \times \frac{1+log(tf_{i,j})}{1+log(l_j)}),tf_{i,j}为词条i在文档j中的出现次数，l_j为文档的长度$

​	通过这个方法，我们可以计算出每个单词的词条权重，具体可看课本359页表16.3。

​	对于决策树的训练过程，首先要建立一个大规模的树结构，然后对这个树结构进行**剪枝**调整到合适的大小。对于剪枝来说，我们需要定义**分支准则**和**停止准则**。分支准则决定树节点中选取何种属性作为当前训练集的分支属性，停止准则决定何时停止决策树的迭代过程，即当前节点的所有数据都具备相同的数据表示形式或具有相同的类别导致当前树节点的类别不能进行进一步的划分，此时就需要停止迭代过程，返回最终的决策树。

​	我们使用的停止准则为信息熵最大增益原则，可以解释为减少不确定性的一个度量，即：

​	$G(a,y)=H(t)-H(t|a)=H(t)-(p_LH(t_L)+p_RH(t_R)),\\a为分支属性，y为属性a的取值，t表示当前处理节点的分布。\\P_L和P_R表示左右子节点的数据元素在父母节点中所占的比例，t_L和t_R表示左右子节点中数据元素的分布。$

​	当迭代停止时，决策树形成叶节点，对叶节点的预测我们需要计算它的最大似然估计并做出平滑。

​	对于树的构建过程，我们需要对每个叶节点按照特定的准则评估其对分类器的作用，每次剪枝都保留那些分类判断贡献较小的叶节点，直到无叶子结点可剪。

​	决策树的优点就是结构清晰，易于理解，整个分类过程非常直观。

#### 3. 最大熵模型

​	我认为书中所讲最大熵模型不太好，需要参考统计学习方法第六章-逻辑斯谛回归与最大熵模型进行学习。

#### 4. 感知机

​	感知机也可以参考李航老师统计学习方法第二章-感知机，可以参考我之前所做笔记-[https://mrsuncodes.github.io/2021/07/18/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E7%AC%AC%E4%BA%8C%E7%AB%A0/](https://mrsuncodes.github.io/2021/07/18/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E7%AC%AC%E4%BA%8C%E7%AB%A0/)

​	感知机做分类的思想主要是将某个类别判断变成二分类问题，因为感知机本质是线性模型。然后我们通过一般感知机的方法可以将某一条曲线通过梯度下降使其更好的拟合到训练数据中，并最终训练好我们的感知机用于测试。思想非常简单。

#### 5. k最近邻分类

​	k近邻分类可以参考统计学习方法第三章-k近邻，我之前所做笔记-[https://mrsuncodes.github.io/2021/07/21/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E7%AC%AC%E4%B8%89%E7%AB%A0/](https://mrsuncodes.github.io/2021/07/21/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E7%AC%AC%E4%B8%89%E7%AB%A0/)

​	k近邻的思想是从现有训练集合中找到k个最相似的对象，然后按照这k个最相似的对象的最大占比类别进行分类，得到新数据的类别。这个算法的主要在于如何确定数据之间的相似度。当然，我们可以直接通过$NLP$领域中最常用的计算相似度的方法-余弦值方法进行计算。这个算法也非常简单。

------

总结：这本书读完了，对于刚入门的$NLPer$来说是个不错的书籍，可以带你认识到自然语言处理的几个比较经典的概念(词法分析、句法分析、平滑等)、应用(文本分类、文本聚类、机器翻译等)、模型($n-gram，HMM等$)以及对应的解决方案，这些方法也是在深度学习全面拥抱自然语言处理之前(2013年之前)我们应用于工业界的方法。当然，书中$n-gram，HMM$等模型也是现在非常重要的模型，需要好好再看看将全书读完后记录下来也是方便我日后再回顾。之后的任务将是将$word2vec,transformer$弄懂弄透，并海量阅读论文找寻论文的思路以及解决方法。

