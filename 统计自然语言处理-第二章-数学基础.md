

### 统计自然语言处理-第二章-数学基础

​	这一章主要讲了概率论、信息论的基础知识，正好这些知识在[Text Mining](https://github.com/MrSunCodes/Data-Mining-UIUC/tree/main/Text%20Mining)以及[Text Retrieval](https://github.com/MrSunCodes/Data-Mining-UIUC/tree/main/Text%20Retrieval)上面学过，顺便复习了一下几个比较重要的概念，虽说现在做NLP基本上都是深度学习，但是统计学的知识是非常重要的，可以帮助我们打好基础，开拓思路。这篇博客主要对重点知识进行了讲解，并有的放矢。

<!--more-->

------

#### 概率论基础

1. ##### 概率空间

   我们设$\Omega$是一个样本空间或基本事件空间，它是基本事件的集合，且既可以是离散的也可以是连续的，包含了所有可能的实验结果。例如抛三次硬币查看其正面朝上还是背面朝上的结果，$\Omega$ = { *HHH,HHT,HTH,HTT,THH,THT,TTH,TTT* }

2. ##### 条件概率和独立性

   这里需要注意两个定义：

   - 不考虑已知知识对概率值的影响，原有的概率值称为事件的先验概率(Prior probability)

   - 加入已知知识后，原有的概率值将发生变化，称为事件的后验概率(posterior probability)

     举个例子，假设事件B的概率已知，那么事件A发生的条件概率为：

     $ P(A|B) = \frac{P(A\cap B)}{P(B)}\ $

     变换一下公式，成为：

     $P(A\cap B) = P(B)P(A|B) = P(A)P(B|A)$

     当然，扩展到多事件(n维), 则公式为：

     $P(A_1\cap A_2\cap ...\cap A_n) = P(A_1)P(A_2|A_1)P(A_3)P(A_1\cap A_12)...P(A_n|\cap_{i=1}^{n-1}A_i)$

3. ##### 贝叶斯定理

   - 由上面的条件概率公式以及链式规则可以得到：

     $$ P(B|A) = \frac{P(B\cap A)}{P(A)}\ = \frac{P(A|B)P(B)}{P(A)}\ $$

   - 因为P(A)是先验概率即常数，可以直接忽略，进而找到**最大的相对可能性**

     $\mathop{\arg\max}_{B}\frac{P(A|B)P(B)}{P(A)}\ =\mathop{\arg\max}_{B}P(A|B)P(B)\ $

   - 若一组互不相交的集合$B_i$对$A$进行划分，且满足$A\subset\cap_iB_i$(即$A$被$B_i$完全划分，$B_i$全部相加即可获得$A$)

     则有$$ P(A) = \sum_{i}P(A|B_i)P(B_i) $$

   - 上述都很好理解，那么接下来就是贝叶斯定理，前提也是n个互不相交的集合$B_i$对$A$进行划分，且$B_i$之间无交集，则通过上述三个式子可推导出贝叶斯定理：

     $$ P(B_j|A) = \frac{P(A|B_j)P(B_j)}{P(A)}\ = \frac{P(A|B_j)P(B_j)}{\sum_{i}P(A|B_i)P(B_i)}\ $$

4. ##### 期望值与方差

   期望值：**平均值** 

   $E(X) = \sum_xp(x)$

   方差：**每个数值与平均值间距离大小**

   $Var(X) = E((X - E(X))^2) = E(X^2)-E^2(X)$

5. **标准分布**

   标准分布主要介绍了二项分布以及**正态分布**，这里只是简单介绍一下前提以及内容。

   - 二项分布

     设成功概率为p，n次独立实验中成功的次数为r的概率为：

     $b(r;n,p) = C_n^rp^r(1-p)^{n-r}$ 其中 $C_n^r = \frac{n!}{(n-r)!r!}\\$

     二项分布的期望值是$np$, 方差为$np(1-p)$

   - 正态分布(高斯分布)

     关于正态分布，可以看看这篇博客，这里不再赘述，因为应用非常广泛。https://baijiahao.baidu.com/s?id=1664992535661542475&wfr=spider&for=pc

   

#### 信息论基础

1. ##### 熵(Entropy)

   熵用来表示单个随机变量的不确定性的均值，熵越大，它的不确定性越大。计算公式为：

   $H(p)=H(X)=-\sum\limits_{x\in X}p(x)log_2p(x)$

   当然，我们可以改一种形式，将负号提到对数$log$里面，则变成了：

   $H(X)=\sum\limits_{x\in X}p(x)log_2\frac{1}{p(x)}$

   在证明过程中，可以灵活运用两个式子。

2. **联合熵和条件熵**

   联合熵：$H(X,Y)=-\sum\limits_{x\in X}\sum\limits_{y\in Y}p(x,y)log_2p(x,y)$
   条件熵：$H(Y|X)=-\sum\limits_{x\in X}\sum\limits_{y\in Y}p(x,y)log_2p(y|x)$

   条件熵可以理解为在已知X的情况下，传输Y额外所需的平均信息量。

   当然，条件熵是有连锁规则的，其规则为：

   $H(X_1,X_2,...,X_n)=H(X_1)+H(X_2|X_1)+H(X_3|X_1,X_2)...+H(X_n|X_1,X_2,...,X_{n-1})$ 

   > 证明方法在第二章39页，方法略

3. **互信息**

   定义互信息：

   $I(X;Y)=H(X)-H(X|Y)=H(Y)-H(Y|X)$

   由此可见，互信息表达了已知Y的值后X的不确定性的减少量(或已知X的值后Y的不确定性的减少量)，用于计算两个随机变量之间共有信息的度量，它具有以下几个特点：

   - 两个随机变量无关时，互信息为0
   - 当变量之间存在依赖关系时，他们的互信息与依赖程度和变量的熵也相关。

   它的计算公式为：

   $I(X;Y)=\sum\limits_{x,y}p(x,y)log_2\frac{p(x,y)}{p(x)p(y)}$

4. **相对熵**

   相对熵又称KL散度,如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 KL 散度（Kullback-Leibler (KL) divergence）来衡量这两个分布的差异。

   > 在机器学习中，P往往用来表示样本的真实分布，比如[1,0,0]表示当前样本属于第一类。Q用来表示模型所预测的分布，比如[0.7,0.2,0.1]
   > 直观的理解就是如果用P来描述样本，那么就非常完美。而用Q来描述样本，虽然可以大致描述，但是不是那么的完美，信息量不足，需要额外的一些“信息增量”才能达到和P一样完美的描述。如果我们的Q通过反复训练，也能完美的描述样本，那么就不再需要额外的“信息增量”，Q等价于P。

   相对熵的计算公式为：

   $D(p||q) = \sum\limits_{x\in X}p(x)log_2\frac{p(x)}{q(x)}$

   且$0log_2(0/q)=0, plog_2(p/0)=\infty$

5. **交叉熵**

   由相对熵，我们可以得到：

   $D(p||q) = \sum\limits_{x\in X}p(x)log\frac{p(x)}{q(x)}=\sum\limits_{x\in X}p(x)log_2p(x)-\sum\limits_{x\in X}p(x)log_2q(x)=-H(p(x))+(-\sum\limits_{x\in X}p(x)log_2q(x))$

   这样，我们就把相对熵分解成p的熵还有后面的交叉熵了。即交叉熵=

   $-\sum\limits_{x\in X}p(x)log_2q(x)$

   将负号写进去可得：

   $\sum\limits_{x\in X}p(x)log_2\frac{1}{q(x)}$

   在评估label以及predicts的差距的时候，使用相对熵是非常好的方式，且由于相对熵=熵+交叉熵，而熵这一部分是不会变的，故优化过程中，只需关注交叉熵即可。
   
   ------
   
   

以上就是第二章全部内容，第二章主要讲的就是一些统计学的方法，包括概率论以及信息论，难点在于概念的理解。

